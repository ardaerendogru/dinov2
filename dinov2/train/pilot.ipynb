{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/arda/dinov2\")\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from dinov2.data.augmentations import DataAugmentationDINO\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with all the images\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = DataAugmentationDINO(\n",
    "            global_crops_scale=(0.4, 1.0),\n",
    "            local_crops_scale=(0.05, 0.4),\n",
    "            local_crops_number=8,\n",
    "        )\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_files = []\n",
    "        valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "        for filename in os.listdir(image_dir):\n",
    "            ext = os.path.splitext(filename)[1].lower()\n",
    "            if ext in valid_extensions:\n",
    "                self.image_files.append(filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image)\n",
    "            \n",
    "        return transformed\n",
    "\n",
    "def collate_data_and_cast(samples_list, dtype):\n",
    "    n_global_crops = 2\n",
    "    n_local_crops = 8\n",
    "\n",
    "    collated_global_crops = torch.stack([s[\"global_crops\"][i] for i in range(n_global_crops) for s in samples_list])\n",
    "    collated_local_crops = torch.stack([s[\"local_crops\"][i] for i in range(n_local_crops) for s in samples_list])\n",
    "\n",
    "    return {\n",
    "        \"collated_global_crops\": collated_global_crops.to(dtype),\n",
    "        \"collated_local_crops\": collated_local_crops.to(dtype),\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "image_dir = \"/home/arda/.cache/kagglehub/datasets/ardaerendoru/gtagta/versions/1/GTA5/GTA5/images\"  # Path to your image directory\n",
    "dataset = CustomImageDataset(image_dir)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=lambda x: collate_data_and_cast(x, torch.float32)\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Load pretrained ResNet50 model\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Split model into layers\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        layer4_output = self.layer4(x)\n",
    "        \n",
    "        pooled = self.avgpool(layer4_output)\n",
    "        embeddings = torch.flatten(pooled, 1)\n",
    "        \n",
    "        return {\n",
    "            'layer4_output': layer4_output,\n",
    "            'embeddings': embeddings\n",
    "        }\n",
    "\n",
    "    \n",
    "teacher = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
    "student = CustomResNet()\n",
    "# Freeze teacher parameters\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "def get_teacher_output(teacher, global_crops, n_global_crops):\n",
    "    with torch.no_grad():\n",
    "        # Process global crops through teacher\n",
    "        x = global_crops\n",
    "        teacher_output = teacher(x)\n",
    "        teacher_cls_tokens = teacher_output['x_norm_clstoken']\n",
    "        \n",
    "        # Split into chunks for each global crop\n",
    "        teacher_cls_tokens = teacher_cls_tokens.chunk(n_global_crops)\n",
    "        \n",
    "        # Concatenate in reverse order to match crops A->B with B->A\n",
    "        teacher_cls_tokens = torch.cat((teacher_cls_tokens[1], teacher_cls_tokens[0]))\n",
    "        teacher_patch_tokens = teacher_output['x_norm_patchtokens']\n",
    "        \n",
    "        return teacher_cls_tokens, teacher_patch_tokens\n",
    "\n",
    "def get_student_output(student, global_crops, local_crops, n_local_crops, n_global_crops):\n",
    "    # Process global crops through student\n",
    "    inputs_for_student_head_list = []\n",
    "    student_global_embeddings = student.backbone(global_crops)['embeddings']\n",
    "    student_local_embeddings = student(local_crops)['embeddings']\n",
    "    inputs_for_student_head_list.append(student_local_embeddings.unsqueeze(0))\n",
    "    inputs_for_student_head_list.append(student_global_embeddings.unsqueeze(0))\n",
    "    ibot_student_patch_tokens = student(global_crops)['layer4_output']\n",
    "    student_outputs = []\n",
    "    for input_tensor in inputs_for_student_head_list:\n",
    "        student_outputs.append(student.dino_head(input_tensor.squeeze(0)))\n",
    "    student_local_cls_tokens_after_head = student_outputs[0]\n",
    "    student_global_cls_tokens_after_head = student_outputs[1]\n",
    "    # Concatenate in reverse order to match teacher\n",
    "    student_cls_tokens_global = torch.cat((student_cls_tokens_global[1], student_cls_tokens_global[0]))\n",
    "    \n",
    "    return student_cls_tokens\n",
    "\n",
    "# Cross entropy loss for comparing teacher and student outputs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer for student model\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for batch in dataloader:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/disk0/arda/dinov2/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/storage/disk0/arda/dinov2/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/storage/disk0/arda/dinov2/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "I20241118 16:41:37 564450 dinov2 config.py:59] git:\n",
      "  sha: e1277af2ba9496fbadf7aec6eba56e8d882d1e35, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20241118 16:41:37 564450 dinov2 config.py:60] config_file: /home/arda/dinov2/dinov2/configs/ssl_default_config.yaml\n",
      "eval: \n",
      "eval_only: False\n",
      "no_resume: False\n",
      "opts: ['train.output_dir=/storage/disk0/arda/dinov2/dinov2/train']\n",
      "output_dir: /storage/disk0/arda/dinov2/dinov2/train\n",
      "I20241118 16:41:37 564450 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20241118 16:41:37 564450 dinov2 config.py:33] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp32\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp32\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp32\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp32\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp32\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp32\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: CustomDataset:root=/home/arda/.cache/kagglehub/datasets/ardaerendoru/gtagta/versions/1/GTA5/GTA5/images\n",
      "  output_dir: /storage/disk0/arda/dinov2/dinov2/train\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_large\n",
      "  patch_size: 16\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "  num_register_tokens: 0\n",
      "  interpolate_antialias: false\n",
      "  interpolate_offset: 0.1\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 224\n",
      "  local_crops_size: 96\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "W20241118 16:41:37 564450 py.warnings warnings.py:109] /home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "\n",
      "W20241118 16:41:37 564450 py.warnings warnings.py:109] /home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "\n",
      "Using cache found in /home/arda/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "I20241118 16:41:39 564450 dinov2 vision_transformer.py:122] using MLP layer as FFN\n",
      "I20241118 16:41:39 564450 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 386\n",
      "I20241118 16:41:39 564450 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO\n",
      "I20241118 16:41:39 564450 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0\n",
      "I20241118 16:41:39 564450 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536\n",
      "I20241118 16:41:39 564450 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256\n",
      "I20241118 16:41:39 564450 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048\n",
      "I20241118 16:41:39 564450 dinov2 ssl_meta_arch.py:84] OPTIONS -- DINO -- applying KOLEO regularization\n",
      "I20241118 16:41:40 564450 dinov2 ssl_meta_arch.py:94] OPTIONS -- IBOT\n",
      "I20241118 16:41:40 564450 dinov2 ssl_meta_arch.py:95] OPTIONS -- IBOT -- loss_weight: 1.0\n",
      "I20241118 16:41:40 564450 dinov2 ssl_meta_arch.py:96] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]\n",
      "I20241118 16:41:40 564450 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5\n",
      "I20241118 16:41:40 564450 dinov2 ssl_meta_arch.py:129] OPTIONS -- IBOT -- head shared with DINO\n",
      "I20241118 16:41:41 564450 dinov2 train.py:311] Model:\n",
      "SSLMetaArch(\n",
      "  (dino_loss): DINOLoss()\n",
      "  (koleo_loss): KoLeoLoss(\n",
      "    (pdist): PairwiseDistance()\n",
      "  )\n",
      "  (ibot_patch_loss): iBOTPatchLoss()\n",
      "  (student): ModuleDict(\n",
      "    (backbone): CustomResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "    (dino_head): DINOHead(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Linear(in_features=2048, out_features=256, bias=True)\n",
      "      )\n",
      "      (last_layer): Linear(in_features=256, out_features=65536, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (teacher): ModuleDict(\n",
      "    (backbone): DinoVisionTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
      "        (norm): Identity()\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0-11): 12 x NestedTensorBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MemEffAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): LayerScale()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): LayerScale()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (head): Identity()\n",
      "    )\n",
      "    (dino_head): DINOHead(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=386, out_features=2048, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Linear(in_features=2048, out_features=256, bias=True)\n",
      "      )\n",
      "      (last_layer): Linear(in_features=256, out_features=65536, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:64] else code branch\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.downsample.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.downsample.1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.0.downsample.1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.1.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer1.2.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.downsample.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.downsample.1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.0.downsample.1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.1.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.2.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer2.3.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.downsample.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.downsample.1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.0.downsample.1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.1.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.2.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.3.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.4.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer3.5.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.downsample.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.downsample.1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.0.downsample.1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.1.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.conv1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.bn1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.bn1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.conv2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.bn2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.bn2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.conv3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.bn3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] layer4.2.bn3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 ssl_meta_arch.py:402] fusing param groups\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:64] else code branch\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0\n",
      "I20241118 16:41:41 564450 dinov2 ssl_meta_arch.py:402] fusing param groups\n",
      "I20241118 16:41:41 564450 dinov2 train.py:107] Schedulers ready.\n",
      "I20241118 16:41:41 564450 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch\n",
      "I20241118 16:41:41 564450 dinov2 augmentations.py:34] ###################################\n",
      "I20241118 16:41:41 564450 dinov2 augmentations.py:35] Using data augmentation parameters:\n",
      "I20241118 16:41:41 564450 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]\n",
      "I20241118 16:41:41 564450 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]\n",
      "I20241118 16:41:41 564450 dinov2 augmentations.py:38] local_crops_number: 8\n",
      "I20241118 16:41:41 564450 dinov2 augmentations.py:39] global_crops_size: 224\n",
      "I20241118 16:41:41 564450 dinov2 augmentations.py:40] local_crops_size: 96\n",
      "I20241118 16:41:41 564450 dinov2 augmentations.py:41] ###################################\n",
      "I20241118 16:41:41 564450 dinov2 loaders.py:133] using dataset: \"CustomDataset:root=/home/arda/.cache/kagglehub/datasets/ardaerendoru/gtagta/versions/1/GTA5/GTA5/images\"\n",
      "I20241118 16:41:41 564450 dinov2 loaders.py:138] # of dataset samples: 2,500\n",
      "I20241118 16:41:41 564450 dinov2 loaders.py:161] sampler: infinite\n",
      "I20241118 16:41:41 564450 dinov2 loaders.py:255] using PyTorch data loader\n",
      "I20241118 16:41:41 564450 dinov2 loaders.py:270] infinite data loader\n",
      "I20241118 16:41:41 564450 dinov2 train.py:225] Starting training from iteration 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/train/train.py\", line 334, in <module>\n",
      "    main(args)\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/train/train.py\", line 329, in main\n",
      "    do_train(cfg, model, resume=not args.no_resume)\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/train/train.py\", line 253, in do_train\n",
      "    loss_dict = model.forward_backward(data, teacher_temp=teacher_temp)\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/train/ssl_meta_arch.py\", line 247, in forward_backward\n",
      "    teacher_dino_softmaxed_centered_list, masked_teacher_ibot_softmaxed_centered = get_teacher_output()\n",
      "  File \"/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/train/ssl_meta_arch.py\", line 178, in get_teacher_output\n",
      "    teacher_backbone_output_dict = self.teacher.backbone(x, is_training=True)\n",
      "  File \"/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/models/vision_transformer.py\", line 325, in forward\n",
      "    ret = self.forward_features(*args, **kwargs)\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/models/vision_transformer.py\", line 258, in forward_features\n",
      "    x = self.prepare_tokens_with_masks(x, masks)\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/models/vision_transformer.py\", line 215, in prepare_tokens_with_masks\n",
      "    x = self.patch_embed(x)\n",
      "  File \"/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/storage/disk0/arda/dinov2/dinov2/layers/patch_embed.py\", line 75, in forward\n",
      "    x = self.proj(x)  # B C H W\n",
      "  File \"/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
