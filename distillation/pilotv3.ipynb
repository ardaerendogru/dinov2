{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/disk0/arda/dinov2/distillation/../../dinov2/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/storage/disk0/arda/dinov2/distillation/../../dinov2/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/storage/disk0/arda/dinov2/distillation/../../dinov2/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "Using cache found in /home/arda/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_3 shape: torch.Size([1, 1024, 14, 14])\n",
      "torch.Size([1, 1536, 16, 16])\n",
      "torch.Size([1, 1536])\n",
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "from models import DINOv2ViT, CustomResNet\n",
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224).to(device)\n",
    "teacher = DINOv2ViT().to(device)\n",
    "\n",
    "\n",
    "# out = teacher(x)\n",
    "# print(out[\"patch_embeddings\"].shape)\n",
    "# print(out[\"embedding\"].shape)\n",
    "# print(out[\"feature_map\"].shape)\n",
    "\n",
    "student = CustomResNet().to(device)\n",
    "out = student(x)\n",
    "print(out[\"dinov2_feature_map\"].shape)\n",
    "print(out[\"embedding\"].shape)\n",
    "print(out[\"contrastive_embeddings\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d696e26ecff4b7abda04f1dc053b276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d0656970474a3481a599b8ed19b4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1f7bd7cd8e422d99c4e3331582fd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91727b023b24d7cb7dd098ddc80f464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datasets.GTA5 import GTA5Dataset\n",
    "import sys\n",
    "sys.path.append('./datasets')  # Add the datasets directory to the Python path\n",
    "\n",
    "from collate_fn import collate_data_and_cast  # Adjusted import statement\n",
    "# from datasets.collate_fn import collate_data_and_cast\n",
    "from dinov2.data.augmentations import DataAugmentationDINO\n",
    "from torch.utils.data import DataLoader\n",
    "from imagenet import ImageNetDataset\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Load configurations\n",
    "with open(\"config/config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Data Transformation\n",
    "data_transform = DataAugmentationDINO(\n",
    "    global_crops_scale=tuple(cfg['data_transform']['global_crops_scale']),\n",
    "    local_crops_scale=tuple(cfg['data_transform']['local_crops_scale']),\n",
    "    local_crops_number=cfg['data_transform']['n_local_crops'],\n",
    "    global_crops_size=tuple(cfg['data_transform']['global_crops_size']),\n",
    "    local_crops_size=tuple(cfg['data_transform']['local_crops_size']),\n",
    ")\n",
    "\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset = ImageNetDataset(type='train', transform=data_transform, num_samples = 5000)\n",
    "test_dataset = ImageNetDataset(type='test', transform=data_transform, num_samples = 500)\n",
    "# Create train and test dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg['data_loader']['batch_size'], \n",
    "    num_workers=cfg['data_loader']['num_workers'],\n",
    "    shuffle=cfg['data_loader']['shuffle'],\n",
    "    collate_fn=collate_data_and_cast\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg['data_loader']['batch_size'],\n",
    "    num_workers=cfg['data_loader']['num_workers'], \n",
    "    shuffle=False,\n",
    "    collate_fn=collate_data_and_cast\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = getattr(torch.optim, cfg['optimizer']['type'])([\n",
    "    {\"params\": student.parameters()},\n",
    "], lr=2.5e-4)\n",
    "\n",
    "# Freeze teacher model\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "100%|██████████| 16/16 [01:01<00:00,  3.82s/it]\n",
      "100%|██████████| 2/2 [00:12<00:00,  6.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Loss: 5.8403\n",
      "Train Feature Similarity: 0.0879\n",
      "Train Embedding Similarity: 0.0714\n",
      "Test Loss: 4.1575\n",
      "Test Similarity: 0.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:03<00:00,  3.98s/it]\n",
      "100%|██████████| 2/2 [00:12<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 5.5363\n",
      "Train Feature Similarity: 0.1243\n",
      "Train Embedding Similarity: 0.0848\n",
      "Test Loss: 3.9246\n",
      "Test Similarity: 0.1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:05<00:00,  4.12s/it]\n",
      " 50%|█████     | 1/2 [00:10<00:10, 10.78s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.nn.functional as F  # Added import for functional operations\n",
    "from torch.cuda.amp import GradScaler, autocast  # Import for mixed precision\n",
    "\n",
    "best_test_similarity = 0\n",
    "save_frequency = 5\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "scaler = GradScaler()\n",
    "\n",
    "def compute_feature_similarity(feat1, feat2):\n",
    "    # Reshape feature maps to 2D: (batch*height*width, channels)\n",
    "    f1 = feat1.reshape(-1, feat1.shape[-1])\n",
    "    f2 = feat2.reshape(-1, feat2.shape[-1])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = torch.nn.functional.cosine_similarity(f1, f2, dim=1)\n",
    "    return similarity.mean()\n",
    "\n",
    "def compound_loss(mse_loss, cosine_sim_loss, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Combine MSE loss and Cosine Similarity loss.\n",
    "    \n",
    "    Args:\n",
    "        mse_loss (torch.Tensor): Mean Squared Error loss.\n",
    "        cosine_sim_loss (torch.Tensor): Cosine Similarity loss.\n",
    "        alpha (float): Weight for MSE loss.\n",
    "        beta (float): Weight for Cosine Similarity loss.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Combined loss.\n",
    "    \"\"\"\n",
    "    return alpha * mse_loss + beta * cosine_sim_loss\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    student.load_state_dict(checkpoint['student_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_test_similarity = checkpoint['best_test_similarity']\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(1000):\n",
    "    epoch_loss = []\n",
    "    similarities = []\n",
    "    embedding_similarities = []\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    for i, data in enumerate(tqdm(train_loader)):\n",
    "        global_crops = data[\"collated_global_crops\"].to(device)\n",
    "        local_crops = data[\"collated_local_crops\"].to(device)\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            # Get feature maps from teacher\n",
    "            with torch.no_grad():\n",
    "                teacher_output = teacher(global_crops)\n",
    "                teacher_feature_maps = teacher_output[\"feature_map\"]\n",
    "                teacher_embedding = teacher_output[\"embedding\"]\n",
    "\n",
    "            # Get feature maps from student\n",
    "            student_output = student(global_crops)\n",
    "            student_feature_maps = student_output[\"dinov2_feature_map\"]\n",
    "            student_embedding = student_output[\"embedding\"]\n",
    "\n",
    "            # Calculate MSE loss between feature maps\n",
    "            mse_loss = torch.nn.functional.mse_loss(\n",
    "                student_feature_maps,\n",
    "                teacher_feature_maps\n",
    "            )\n",
    "            mse_embedding_loss = torch.nn.functional.mse_loss(\n",
    "                student_embedding,\n",
    "                teacher_embedding\n",
    "            )\n",
    "            \n",
    "            # Calculate Cosine Similarity loss\n",
    "            student_feature_normalized = F.normalize(student_feature_maps, p=2, dim=1)\n",
    "            teacher_feature_normalized = F.normalize(teacher_feature_maps, p=2, dim=1)\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                student_feature_normalized, \n",
    "                teacher_feature_normalized, \n",
    "                dim=1\n",
    "            )\n",
    "            cosine_similarity_loss = 1 - cosine_similarity.mean()  # Convert similarity to loss\n",
    "\n",
    "            student_embedding_normalized = F.normalize(student_embedding, p=2, dim=1)\n",
    "            teacher_embedding_normalized = F.normalize(teacher_embedding, p=2, dim=1)\n",
    "            cosine_similarity_embedding = torch.nn.functional.cosine_similarity(\n",
    "                student_embedding_normalized, \n",
    "                teacher_embedding_normalized, \n",
    "                dim=1\n",
    "            )\n",
    "            cosine_similarity_embedding_loss = 1 - cosine_similarity_embedding.mean()  # Convert similarity to loss\n",
    "\n",
    "            # Combine the losses\n",
    "            total_loss = compound_loss(mse_loss, cosine_similarity_loss, alpha=1.0, beta=1.0)\n",
    "            total_embedding_loss = compound_loss(mse_embedding_loss, cosine_similarity_embedding_loss, alpha=1.0, beta=1.0)\n",
    "            total_loss += total_embedding_loss\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate similarity for logging\n",
    "        similarity = compute_feature_similarity(student_feature_maps, teacher_feature_maps)\n",
    "        similarities.append(similarity.item())\n",
    "        # Calculate embedding similarity for logging\n",
    "        embedding_similarity = compute_feature_similarity(student_embedding, teacher_embedding)\n",
    "        embedding_similarities.append(embedding_similarity.item())\n",
    "        \n",
    "        epoch_loss.append(total_loss.item())\n",
    "\n",
    "    # Evaluation on test set\n",
    "    student.eval()\n",
    "    test_losses = []\n",
    "    test_similarities = []\n",
    "    test_embedding_similarities = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(test_loader)):\n",
    "            global_crops = data[\"collated_global_crops\"].to(device)\n",
    "            \n",
    "            teacher_output = teacher(global_crops)\n",
    "            student_output = student(global_crops)\n",
    "            \n",
    "            # Feature map losses\n",
    "            test_mse = torch.nn.functional.mse_loss(\n",
    "                student_output[\"dinov2_feature_map\"],\n",
    "                teacher_output[\"feature_map\"]\n",
    "            )\n",
    "            test_similarity = compute_feature_similarity(\n",
    "                student_output[\"dinov2_feature_map\"],\n",
    "                teacher_output[\"feature_map\"]\n",
    "            )\n",
    "            \n",
    "            # Embedding losses\n",
    "            test_embedding_mse = torch.nn.functional.mse_loss(\n",
    "                student_output[\"embedding\"],\n",
    "                teacher_output[\"embedding\"]\n",
    "            )\n",
    "            test_embedding_similarity = compute_feature_similarity(\n",
    "                student_output[\"embedding\"],\n",
    "                teacher_output[\"embedding\"]\n",
    "            )\n",
    "            \n",
    "            test_losses.append(test_mse.item() + test_embedding_mse.item())\n",
    "            test_similarities.append(test_similarity.item())\n",
    "            test_embedding_similarities.append(test_embedding_similarity.item())\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_train_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    avg_train_similarity = sum(similarities)/len(similarities)\n",
    "    avg_train_embedding_similarity = sum(embedding_similarities)/len(embedding_similarities)\n",
    "    avg_test_loss = sum(test_losses)/len(test_losses)\n",
    "    avg_test_similarity = sum(test_similarities)/len(test_similarities)\n",
    "    avg_test_embedding_similarity = sum(test_embedding_similarities)/len(test_embedding_similarities)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Train Feature Similarity: {avg_train_similarity:.4f}\")\n",
    "    print(f\"Train Embedding Similarity: {avg_train_embedding_similarity:.4f}\")\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Test Feature Similarity: {avg_test_similarity:.4f}\")\n",
    "    print(f\"Test Embedding Similarity: {avg_test_embedding_similarity:.4f}\")\n",
    "\n",
    "     # Save checkpoint\n",
    "    if (epoch + 1) % save_frequency == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'student_state_dict': student.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'test_loss': avg_test_loss,\n",
    "            'train_feature_similarity': avg_train_similarity,\n",
    "            'train_embedding_similarity': avg_train_embedding_similarity,\n",
    "            'test_similarity': avg_test_similarity,\n",
    "            'best_test_similarity': best_test_similarity\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\"))\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "        \n",
    "    # Save best model\n",
    "    if avg_test_similarity > best_test_similarity:\n",
    "        best_test_similarity = avg_test_similarity\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'student_state_dict': student.state_dict(),\n",
    "            'test_similarity': avg_test_similarity,\n",
    "            'test_embedding_similarity': avg_test_similarity\n",
    "        }, os.path.join(checkpoint_dir, \"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.arange(32).repeat(2)  # Creates [0,1,2,...,batch_size-1, 0,1,2,...,batch_size-1]\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  0,  1,  2,  3,\n",
       "         4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31], device='cuda:1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
