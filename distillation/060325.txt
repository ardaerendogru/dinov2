1- Validation dataset (for example training on sam and validate on coco---> i can make list of folders training instead of one folder)
2- Cleaned the codebase, some important things are:
    2.1- There was a projection layer between features and scalekd at first which is from direct mse los, removed it (now we have only interpolation+scalekd)
    2.2- I realized that I hardcoded student dimensions for the projector (2048 resnet) which i removed (stdc was 1024), so it was like expanding channels and reducing it after for stdc. Fixed it.
    2.3- Fixed checkpoint saving and some other little things, but when we continue to training it logs to a new directory,I need to fix that, also it creates a new wandb exp.
3- Augmentations: ScaleKD uses a different augmentation pipeline (RandAugment paper + random erasing), It improves similarity score and decreases loss on both train on val
4- There is a paper which is very simple, they are using 3 projection heads from student to teacher and it improves performance (it is like an ensemble), it might justify the frequency path loss
5- ScaleKD paper misleaded me, in the code they input dinov2 from the 9th block and having output of 11th (12 blocks in total). I was having output of 12th block for the tpp part (In their code we are comparing 
dinov2 outputs vs dinov2 11th block output which we get cnn features, why?) -- Fixed it and it increases performance (similarity)
6- Anyma backbones import some other function/classes from anyma codebase, i didn't check it detail, is there any easy way?
7- Created a script for converting distillation weights to anyma
8- Completed docstrings.
9- Found optimal lr and made batch size higher
9- TODO: Logging maybe like in the anyma?