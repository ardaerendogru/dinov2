Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Starting training from scratch.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/storage/disk0/arda/dinov2/distillation/train.py", line 200, in <module>
[rank0]:     main()
[rank0]:   File "/storage/disk0/arda/dinov2/distillation/train.py", line 196, in main
[rank0]:     trainer.train()
[rank0]:   File "/storage/disk0/arda/dinov2/distillation/train.py", line 157, in train
[rank0]:     self.trainer.fit(self.distillation_module, self.data_module)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 957, in _run
[rank0]:     self.strategy.setup(self)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 171, in setup
[rank0]:     self.configure_ddp()
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 283, in configure_ddp
[rank0]:     self.model = self._setup_model(self.model)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 195, in _setup_model
[rank0]:     return DistributedDataParallel(module=model, device_ids=device_ids, **self._ddp_kwargs)
[rank0]: TypeError: DistributedDataParallel.__init__() got an unexpected keyword argument 'find_unused_parameters_fn'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/storage/disk0/arda/dinov2/distillation/train.py", line 200, in <module>
[rank0]:     main()
[rank0]:   File "/storage/disk0/arda/dinov2/distillation/train.py", line 196, in main
[rank0]:     trainer.train()
[rank0]:   File "/storage/disk0/arda/dinov2/distillation/train.py", line 157, in train
[rank0]:     self.trainer.fit(self.distillation_module, self.data_module)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 957, in _run
[rank0]:     self.strategy.setup(self)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 171, in setup
[rank0]:     self.configure_ddp()
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 283, in configure_ddp
[rank0]:     self.model = self._setup_model(self.model)
[rank0]:   File "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 195, in _setup_model
[rank0]:     return DistributedDataParallel(module=model, device_ids=device_ids, **self._ddp_kwargs)
[rank0]: TypeError: DistributedDataParallel.__init__() got an unexpected keyword argument 'find_unused_parameters_fn'
