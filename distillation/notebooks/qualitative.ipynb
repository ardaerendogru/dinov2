{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/arda/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1+cu124 with CUDA 1204 (you have 2.0.0+cu117)\n",
      "    Python  3.9.20 (you have 3.9.20)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/home/arda/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/arda/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/arda/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the model\n",
    "dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "dinov2.eval()\n",
    "\n",
    "# Define image transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Helper function to load and process an image\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "# Example usage\n",
    "# image = load_image('path/to/your/image.jpg')\n",
    "# with torch.no_grad():\n",
    "#     features = dinov2(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/arda/dinov2')\n",
    "from distillation.datasets.CustomDataset import CustomDataset\n",
    "\n",
    "dataset = CustomDataset(img_dir='/home/arda/data/train2017', transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8782, -1.8782, -1.8782,  ..., -0.4054, -0.4054, -0.3883],\n",
       "         [-1.8610, -1.8610, -1.8782,  ..., -0.3883, -0.3541, -0.3369],\n",
       "         [-1.8610, -1.8782, -1.8782,  ..., -0.3541, -0.3027, -0.3027],\n",
       "         ...,\n",
       "         [ 0.0569,  0.0398,  0.0398,  ..., -0.1486, -0.1657, -0.2513],\n",
       "         [ 0.0227,  0.0227,  0.0398,  ..., -0.1657, -0.1999, -0.2513],\n",
       "         [ 0.0227,  0.0569,  0.1083,  ..., -0.1999, -0.2171, -0.2684]],\n",
       "\n",
       "        [[-1.8431, -1.8431, -1.8431,  ...,  0.3803,  0.3803,  0.4153],\n",
       "         [-1.8431, -1.8431, -1.8431,  ...,  0.3978,  0.4153,  0.4153],\n",
       "         [-1.8431, -1.8431, -1.8431,  ...,  0.3978,  0.3978,  0.4153],\n",
       "         ...,\n",
       "         [ 0.5903,  0.6078,  0.6078,  ...,  0.2402,  0.1877,  0.1877],\n",
       "         [ 0.6078,  0.6254,  0.6254,  ...,  0.2227,  0.2052,  0.1877],\n",
       "         [ 0.6078,  0.6254,  0.6604,  ...,  0.2227,  0.2052,  0.2052]],\n",
       "\n",
       "        [[-1.4210, -1.4384, -1.4384,  ...,  0.7054,  0.6879,  0.6356],\n",
       "         [-1.4210, -1.4210, -1.4210,  ...,  0.6879,  0.6879,  0.6182],\n",
       "         [-1.4733, -1.4559, -1.4384,  ...,  0.6705,  0.7228,  0.6879],\n",
       "         ...,\n",
       "         [ 0.9319,  0.9319,  0.9145,  ...,  0.4091,  0.3742,  0.3742],\n",
       "         [ 0.9145,  0.8971,  0.9145,  ...,  0.3742,  0.3742,  0.3393],\n",
       "         [ 0.9145,  0.9145,  0.9494,  ...,  0.4265,  0.4091,  0.3742]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distillation.models.ModelWrapper import ModelWrapper\n",
    "\n",
    "resnet50 = ModelWrapper(\n",
    "            model_type='resnet',\n",
    "            n_patches=256,\n",
    "            target_feature=['res5'],\n",
    "            feature_matcher_config=None,\n",
    "            checkpoint_path='/home/arda/dinov2/distillation/checkpoints/resnet50_distilled.pth',\n",
    "            **{'depth': 50, 'out_features': ['res4', 'res5'], 'freeze_at': 0, 'norm_type': 'BN'}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
