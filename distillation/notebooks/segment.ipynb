{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': tensor(49.4986), 'mse': tensor(97.9973), 'cosine': tensor(1.0000)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class DistillationLoss:\n",
    "    \"\"\"Handles all loss computations for distillation.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, beta=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weight for MSE loss\n",
    "            beta: Weight for cosine loss\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        # Option 1: Normalize by sum of coefficients\n",
    "        self.normalizer = alpha + beta if (alpha + beta) > 0 else 1.0\n",
    "\n",
    "    def __call__(self, student_features, teacher_features):\n",
    "        \"\"\"Compute all losses and return as dictionary.\"\"\"\n",
    "\n",
    "        student_norm = F.normalize(student_features, dim=1)\n",
    "        teacher_norm = F.normalize(teacher_features, dim=1)\n",
    "        N,C,H,W = student_norm.shape\n",
    "        # MSE on normalized features\n",
    "        mse = nn.MSELoss(reduction='sum')\n",
    "        mse_loss = mse(student_norm, teacher_norm)/N\n",
    "\n",
    "        cosine_sim = F.cosine_similarity(student_norm, teacher_norm, dim=1)\n",
    "        cosine_loss = 1 - cosine_sim.mean()\n",
    "\n",
    "\n",
    "        # Option 1: Normalize by sum of coefficients (recommended)\n",
    "        total_loss = (self.alpha * mse_loss + self.beta * cosine_loss) / self.normalizer\n",
    "\n",
    "\n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'mse': mse_loss,\n",
    "            'cosine': cosine_loss\n",
    "        }\n",
    "\n",
    "loss = DistillationLoss()\n",
    "student_features = torch.randn((32,2048,7,7))\n",
    "teacher_features = torch.randn((32,2048,7,7))\n",
    "\n",
    "loss(student_features, teacher_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': tensor(1.4989), 'mse': tensor(1.9981), 'cosine': tensor(0.9998)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DistillationLoss:\n",
    "    \"\"\"Handles all loss computations for distillation.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, beta=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weight for MSE loss\n",
    "            beta: Weight for cosine loss\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        # Option 1: Normalize by sum of coefficients\n",
    "        self.normalizer = alpha + beta if (alpha + beta) > 0 else 1.0\n",
    "\n",
    "    def __call__(self, student_features, teacher_features):\n",
    "        \"\"\"Compute all losses and return as dictionary.\"\"\"\n",
    "        mse_loss = F.mse_loss(student_features, teacher_features)\n",
    "        \n",
    "        # Cosine similarity loss\n",
    "        student_norm = F.normalize(student_features, p=2, dim=1)\n",
    "        teacher_norm = F.normalize(teacher_features, p=2, dim=1)\n",
    "        cosine_sim = F.cosine_similarity(student_norm, teacher_norm, dim=1)\n",
    "        cosine_loss = 1 - cosine_sim.mean()\n",
    "\n",
    "\n",
    "        # Option 1: Normalize by sum of coefficients (recommended)\n",
    "        total_loss = (self.alpha * mse_loss + self.beta * cosine_loss) / self.normalizer\n",
    "\n",
    "\n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'mse': mse_loss,\n",
    "            'cosine': cosine_loss\n",
    "        }\n",
    "loss = DistillationLoss()\n",
    "student_features = torch.randn((32,2048,7,7))\n",
    "teacher_features = torch.randn((32,2048,7,7))\n",
    "\n",
    "loss(student_features, teacher_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.features.0.conv.weight', 'model.features.0.bn.weight', 'model.features.0.bn.bias', 'model.features.0.bn.running_mean', 'model.features.0.bn.running_var', 'model.features.0.bn.num_batches_tracked', 'model.features.1.conv.weight', 'model.features.1.bn.weight', 'model.features.1.bn.bias', 'model.features.1.bn.running_mean', 'model.features.1.bn.running_var', 'model.features.1.bn.num_batches_tracked', 'model.features.2.conv_list.0.conv.weight', 'model.features.2.conv_list.0.bn.weight', 'model.features.2.conv_list.0.bn.bias', 'model.features.2.conv_list.0.bn.running_mean', 'model.features.2.conv_list.0.bn.running_var', 'model.features.2.conv_list.0.bn.num_batches_tracked', 'model.features.2.conv_list.1.conv.weight', 'model.features.2.conv_list.1.bn.weight', 'model.features.2.conv_list.1.bn.bias', 'model.features.2.conv_list.1.bn.running_mean', 'model.features.2.conv_list.1.bn.running_var', 'model.features.2.conv_list.1.bn.num_batches_tracked', 'model.features.2.conv_list.2.conv.weight', 'model.features.2.conv_list.2.bn.weight', 'model.features.2.conv_list.2.bn.bias', 'model.features.2.conv_list.2.bn.running_mean', 'model.features.2.conv_list.2.bn.running_var', 'model.features.2.conv_list.2.bn.num_batches_tracked', 'model.features.2.conv_list.3.conv.weight', 'model.features.2.conv_list.3.bn.weight', 'model.features.2.conv_list.3.bn.bias', 'model.features.2.conv_list.3.bn.running_mean', 'model.features.2.conv_list.3.bn.running_var', 'model.features.2.conv_list.3.bn.num_batches_tracked', 'model.features.2.avd_layer.0.weight', 'model.features.2.avd_layer.1.weight', 'model.features.2.avd_layer.1.bias', 'model.features.2.avd_layer.1.running_mean', 'model.features.2.avd_layer.1.running_var', 'model.features.2.avd_layer.1.num_batches_tracked', 'model.features.3.conv_list.0.conv.weight', 'model.features.3.conv_list.0.bn.weight', 'model.features.3.conv_list.0.bn.bias', 'model.features.3.conv_list.0.bn.running_mean', 'model.features.3.conv_list.0.bn.running_var', 'model.features.3.conv_list.0.bn.num_batches_tracked', 'model.features.3.conv_list.1.conv.weight', 'model.features.3.conv_list.1.bn.weight', 'model.features.3.conv_list.1.bn.bias', 'model.features.3.conv_list.1.bn.running_mean', 'model.features.3.conv_list.1.bn.running_var', 'model.features.3.conv_list.1.bn.num_batches_tracked', 'model.features.3.conv_list.2.conv.weight', 'model.features.3.conv_list.2.bn.weight', 'model.features.3.conv_list.2.bn.bias', 'model.features.3.conv_list.2.bn.running_mean', 'model.features.3.conv_list.2.bn.running_var', 'model.features.3.conv_list.2.bn.num_batches_tracked', 'model.features.3.conv_list.3.conv.weight', 'model.features.3.conv_list.3.bn.weight', 'model.features.3.conv_list.3.bn.bias', 'model.features.3.conv_list.3.bn.running_mean', 'model.features.3.conv_list.3.bn.running_var', 'model.features.3.conv_list.3.bn.num_batches_tracked', 'model.features.4.conv_list.0.conv.weight', 'model.features.4.conv_list.0.bn.weight', 'model.features.4.conv_list.0.bn.bias', 'model.features.4.conv_list.0.bn.running_mean', 'model.features.4.conv_list.0.bn.running_var', 'model.features.4.conv_list.0.bn.num_batches_tracked', 'model.features.4.conv_list.1.conv.weight', 'model.features.4.conv_list.1.bn.weight', 'model.features.4.conv_list.1.bn.bias', 'model.features.4.conv_list.1.bn.running_mean', 'model.features.4.conv_list.1.bn.running_var', 'model.features.4.conv_list.1.bn.num_batches_tracked', 'model.features.4.conv_list.2.conv.weight', 'model.features.4.conv_list.2.bn.weight', 'model.features.4.conv_list.2.bn.bias', 'model.features.4.conv_list.2.bn.running_mean', 'model.features.4.conv_list.2.bn.running_var', 'model.features.4.conv_list.2.bn.num_batches_tracked', 'model.features.4.conv_list.3.conv.weight', 'model.features.4.conv_list.3.bn.weight', 'model.features.4.conv_list.3.bn.bias', 'model.features.4.conv_list.3.bn.running_mean', 'model.features.4.conv_list.3.bn.running_var', 'model.features.4.conv_list.3.bn.num_batches_tracked', 'model.features.5.conv_list.0.conv.weight', 'model.features.5.conv_list.0.bn.weight', 'model.features.5.conv_list.0.bn.bias', 'model.features.5.conv_list.0.bn.running_mean', 'model.features.5.conv_list.0.bn.running_var', 'model.features.5.conv_list.0.bn.num_batches_tracked', 'model.features.5.conv_list.1.conv.weight', 'model.features.5.conv_list.1.bn.weight', 'model.features.5.conv_list.1.bn.bias', 'model.features.5.conv_list.1.bn.running_mean', 'model.features.5.conv_list.1.bn.running_var', 'model.features.5.conv_list.1.bn.num_batches_tracked', 'model.features.5.conv_list.2.conv.weight', 'model.features.5.conv_list.2.bn.weight', 'model.features.5.conv_list.2.bn.bias', 'model.features.5.conv_list.2.bn.running_mean', 'model.features.5.conv_list.2.bn.running_var', 'model.features.5.conv_list.2.bn.num_batches_tracked', 'model.features.5.conv_list.3.conv.weight', 'model.features.5.conv_list.3.bn.weight', 'model.features.5.conv_list.3.bn.bias', 'model.features.5.conv_list.3.bn.running_mean', 'model.features.5.conv_list.3.bn.running_var', 'model.features.5.conv_list.3.bn.num_batches_tracked', 'model.features.6.conv_list.0.conv.weight', 'model.features.6.conv_list.0.bn.weight', 'model.features.6.conv_list.0.bn.bias', 'model.features.6.conv_list.0.bn.running_mean', 'model.features.6.conv_list.0.bn.running_var', 'model.features.6.conv_list.0.bn.num_batches_tracked', 'model.features.6.conv_list.1.conv.weight', 'model.features.6.conv_list.1.bn.weight', 'model.features.6.conv_list.1.bn.bias', 'model.features.6.conv_list.1.bn.running_mean', 'model.features.6.conv_list.1.bn.running_var', 'model.features.6.conv_list.1.bn.num_batches_tracked', 'model.features.6.conv_list.2.conv.weight', 'model.features.6.conv_list.2.bn.weight', 'model.features.6.conv_list.2.bn.bias', 'model.features.6.conv_list.2.bn.running_mean', 'model.features.6.conv_list.2.bn.running_var', 'model.features.6.conv_list.2.bn.num_batches_tracked', 'model.features.6.conv_list.3.conv.weight', 'model.features.6.conv_list.3.bn.weight', 'model.features.6.conv_list.3.bn.bias', 'model.features.6.conv_list.3.bn.running_mean', 'model.features.6.conv_list.3.bn.running_var', 'model.features.6.conv_list.3.bn.num_batches_tracked', 'model.features.6.avd_layer.0.weight', 'model.features.6.avd_layer.1.weight', 'model.features.6.avd_layer.1.bias', 'model.features.6.avd_layer.1.running_mean', 'model.features.6.avd_layer.1.running_var', 'model.features.6.avd_layer.1.num_batches_tracked', 'model.features.7.conv_list.0.conv.weight', 'model.features.7.conv_list.0.bn.weight', 'model.features.7.conv_list.0.bn.bias', 'model.features.7.conv_list.0.bn.running_mean', 'model.features.7.conv_list.0.bn.running_var', 'model.features.7.conv_list.0.bn.num_batches_tracked', 'model.features.7.conv_list.1.conv.weight', 'model.features.7.conv_list.1.bn.weight', 'model.features.7.conv_list.1.bn.bias', 'model.features.7.conv_list.1.bn.running_mean', 'model.features.7.conv_list.1.bn.running_var', 'model.features.7.conv_list.1.bn.num_batches_tracked', 'model.features.7.conv_list.2.conv.weight', 'model.features.7.conv_list.2.bn.weight', 'model.features.7.conv_list.2.bn.bias', 'model.features.7.conv_list.2.bn.running_mean', 'model.features.7.conv_list.2.bn.running_var', 'model.features.7.conv_list.2.bn.num_batches_tracked', 'model.features.7.conv_list.3.conv.weight', 'model.features.7.conv_list.3.bn.weight', 'model.features.7.conv_list.3.bn.bias', 'model.features.7.conv_list.3.bn.running_mean', 'model.features.7.conv_list.3.bn.running_var', 'model.features.7.conv_list.3.bn.num_batches_tracked', 'model.features.8.conv_list.0.conv.weight', 'model.features.8.conv_list.0.bn.weight', 'model.features.8.conv_list.0.bn.bias', 'model.features.8.conv_list.0.bn.running_mean', 'model.features.8.conv_list.0.bn.running_var', 'model.features.8.conv_list.0.bn.num_batches_tracked', 'model.features.8.conv_list.1.conv.weight', 'model.features.8.conv_list.1.bn.weight', 'model.features.8.conv_list.1.bn.bias', 'model.features.8.conv_list.1.bn.running_mean', 'model.features.8.conv_list.1.bn.running_var', 'model.features.8.conv_list.1.bn.num_batches_tracked', 'model.features.8.conv_list.2.conv.weight', 'model.features.8.conv_list.2.bn.weight', 'model.features.8.conv_list.2.bn.bias', 'model.features.8.conv_list.2.bn.running_mean', 'model.features.8.conv_list.2.bn.running_var', 'model.features.8.conv_list.2.bn.num_batches_tracked', 'model.features.8.conv_list.3.conv.weight', 'model.features.8.conv_list.3.bn.weight', 'model.features.8.conv_list.3.bn.bias', 'model.features.8.conv_list.3.bn.running_mean', 'model.features.8.conv_list.3.bn.running_var', 'model.features.8.conv_list.3.bn.num_batches_tracked', 'model.features.9.conv_list.0.conv.weight', 'model.features.9.conv_list.0.bn.weight', 'model.features.9.conv_list.0.bn.bias', 'model.features.9.conv_list.0.bn.running_mean', 'model.features.9.conv_list.0.bn.running_var', 'model.features.9.conv_list.0.bn.num_batches_tracked', 'model.features.9.conv_list.1.conv.weight', 'model.features.9.conv_list.1.bn.weight', 'model.features.9.conv_list.1.bn.bias', 'model.features.9.conv_list.1.bn.running_mean', 'model.features.9.conv_list.1.bn.running_var', 'model.features.9.conv_list.1.bn.num_batches_tracked', 'model.features.9.conv_list.2.conv.weight', 'model.features.9.conv_list.2.bn.weight', 'model.features.9.conv_list.2.bn.bias', 'model.features.9.conv_list.2.bn.running_mean', 'model.features.9.conv_list.2.bn.running_var', 'model.features.9.conv_list.2.bn.num_batches_tracked', 'model.features.9.conv_list.3.conv.weight', 'model.features.9.conv_list.3.bn.weight', 'model.features.9.conv_list.3.bn.bias', 'model.features.9.conv_list.3.bn.running_mean', 'model.features.9.conv_list.3.bn.running_var', 'model.features.9.conv_list.3.bn.num_batches_tracked', 'model.features.10.conv_list.0.conv.weight', 'model.features.10.conv_list.0.bn.weight', 'model.features.10.conv_list.0.bn.bias', 'model.features.10.conv_list.0.bn.running_mean', 'model.features.10.conv_list.0.bn.running_var', 'model.features.10.conv_list.0.bn.num_batches_tracked', 'model.features.10.conv_list.1.conv.weight', 'model.features.10.conv_list.1.bn.weight', 'model.features.10.conv_list.1.bn.bias', 'model.features.10.conv_list.1.bn.running_mean', 'model.features.10.conv_list.1.bn.running_var', 'model.features.10.conv_list.1.bn.num_batches_tracked', 'model.features.10.conv_list.2.conv.weight', 'model.features.10.conv_list.2.bn.weight', 'model.features.10.conv_list.2.bn.bias', 'model.features.10.conv_list.2.bn.running_mean', 'model.features.10.conv_list.2.bn.running_var', 'model.features.10.conv_list.2.bn.num_batches_tracked', 'model.features.10.conv_list.3.conv.weight', 'model.features.10.conv_list.3.bn.weight', 'model.features.10.conv_list.3.bn.bias', 'model.features.10.conv_list.3.bn.running_mean', 'model.features.10.conv_list.3.bn.running_var', 'model.features.10.conv_list.3.bn.num_batches_tracked', 'model.features.11.conv_list.0.conv.weight', 'model.features.11.conv_list.0.bn.weight', 'model.features.11.conv_list.0.bn.bias', 'model.features.11.conv_list.0.bn.running_mean', 'model.features.11.conv_list.0.bn.running_var', 'model.features.11.conv_list.0.bn.num_batches_tracked', 'model.features.11.conv_list.1.conv.weight', 'model.features.11.conv_list.1.bn.weight', 'model.features.11.conv_list.1.bn.bias', 'model.features.11.conv_list.1.bn.running_mean', 'model.features.11.conv_list.1.bn.running_var', 'model.features.11.conv_list.1.bn.num_batches_tracked', 'model.features.11.conv_list.2.conv.weight', 'model.features.11.conv_list.2.bn.weight', 'model.features.11.conv_list.2.bn.bias', 'model.features.11.conv_list.2.bn.running_mean', 'model.features.11.conv_list.2.bn.running_var', 'model.features.11.conv_list.2.bn.num_batches_tracked', 'model.features.11.conv_list.3.conv.weight', 'model.features.11.conv_list.3.bn.weight', 'model.features.11.conv_list.3.bn.bias', 'model.features.11.conv_list.3.bn.running_mean', 'model.features.11.conv_list.3.bn.running_var', 'model.features.11.conv_list.3.bn.num_batches_tracked', 'model.features.11.avd_layer.0.weight', 'model.features.11.avd_layer.1.weight', 'model.features.11.avd_layer.1.bias', 'model.features.11.avd_layer.1.running_mean', 'model.features.11.avd_layer.1.running_var', 'model.features.11.avd_layer.1.num_batches_tracked', 'model.features.12.conv_list.0.conv.weight', 'model.features.12.conv_list.0.bn.weight', 'model.features.12.conv_list.0.bn.bias', 'model.features.12.conv_list.0.bn.running_mean', 'model.features.12.conv_list.0.bn.running_var', 'model.features.12.conv_list.0.bn.num_batches_tracked', 'model.features.12.conv_list.1.conv.weight', 'model.features.12.conv_list.1.bn.weight', 'model.features.12.conv_list.1.bn.bias', 'model.features.12.conv_list.1.bn.running_mean', 'model.features.12.conv_list.1.bn.running_var', 'model.features.12.conv_list.1.bn.num_batches_tracked', 'model.features.12.conv_list.2.conv.weight', 'model.features.12.conv_list.2.bn.weight', 'model.features.12.conv_list.2.bn.bias', 'model.features.12.conv_list.2.bn.running_mean', 'model.features.12.conv_list.2.bn.running_var', 'model.features.12.conv_list.2.bn.num_batches_tracked', 'model.features.12.conv_list.3.conv.weight', 'model.features.12.conv_list.3.bn.weight', 'model.features.12.conv_list.3.bn.bias', 'model.features.12.conv_list.3.bn.running_mean', 'model.features.12.conv_list.3.bn.running_var', 'model.features.12.conv_list.3.bn.num_batches_tracked', 'model.features.13.conv_list.0.conv.weight', 'model.features.13.conv_list.0.bn.weight', 'model.features.13.conv_list.0.bn.bias', 'model.features.13.conv_list.0.bn.running_mean', 'model.features.13.conv_list.0.bn.running_var', 'model.features.13.conv_list.0.bn.num_batches_tracked', 'model.features.13.conv_list.1.conv.weight', 'model.features.13.conv_list.1.bn.weight', 'model.features.13.conv_list.1.bn.bias', 'model.features.13.conv_list.1.bn.running_mean', 'model.features.13.conv_list.1.bn.running_var', 'model.features.13.conv_list.1.bn.num_batches_tracked', 'model.features.13.conv_list.2.conv.weight', 'model.features.13.conv_list.2.bn.weight', 'model.features.13.conv_list.2.bn.bias', 'model.features.13.conv_list.2.bn.running_mean', 'model.features.13.conv_list.2.bn.running_var', 'model.features.13.conv_list.2.bn.num_batches_tracked', 'model.features.13.conv_list.3.conv.weight', 'model.features.13.conv_list.3.bn.weight', 'model.features.13.conv_list.3.bn.bias', 'model.features.13.conv_list.3.bn.running_mean', 'model.features.13.conv_list.3.bn.running_var', 'model.features.13.conv_list.3.bn.num_batches_tracked', 'model.x2.0.0.conv.weight', 'model.x2.0.0.bn.weight', 'model.x2.0.0.bn.bias', 'model.x2.0.0.bn.running_mean', 'model.x2.0.0.bn.running_var', 'model.x2.0.0.bn.num_batches_tracked', 'model.x4.0.1.conv.weight', 'model.x4.0.1.bn.weight', 'model.x4.0.1.bn.bias', 'model.x4.0.1.bn.running_mean', 'model.x4.0.1.bn.running_var', 'model.x4.0.1.bn.num_batches_tracked', 'model.x8.0.2.conv_list.0.conv.weight', 'model.x8.0.2.conv_list.0.bn.weight', 'model.x8.0.2.conv_list.0.bn.bias', 'model.x8.0.2.conv_list.0.bn.running_mean', 'model.x8.0.2.conv_list.0.bn.running_var', 'model.x8.0.2.conv_list.0.bn.num_batches_tracked', 'model.x8.0.2.conv_list.1.conv.weight', 'model.x8.0.2.conv_list.1.bn.weight', 'model.x8.0.2.conv_list.1.bn.bias', 'model.x8.0.2.conv_list.1.bn.running_mean', 'model.x8.0.2.conv_list.1.bn.running_var', 'model.x8.0.2.conv_list.1.bn.num_batches_tracked', 'model.x8.0.2.conv_list.2.conv.weight', 'model.x8.0.2.conv_list.2.bn.weight', 'model.x8.0.2.conv_list.2.bn.bias', 'model.x8.0.2.conv_list.2.bn.running_mean', 'model.x8.0.2.conv_list.2.bn.running_var', 'model.x8.0.2.conv_list.2.bn.num_batches_tracked', 'model.x8.0.2.conv_list.3.conv.weight', 'model.x8.0.2.conv_list.3.bn.weight', 'model.x8.0.2.conv_list.3.bn.bias', 'model.x8.0.2.conv_list.3.bn.running_mean', 'model.x8.0.2.conv_list.3.bn.running_var', 'model.x8.0.2.conv_list.3.bn.num_batches_tracked', 'model.x8.0.2.avd_layer.0.weight', 'model.x8.0.2.avd_layer.1.weight', 'model.x8.0.2.avd_layer.1.bias', 'model.x8.0.2.avd_layer.1.running_mean', 'model.x8.0.2.avd_layer.1.running_var', 'model.x8.0.2.avd_layer.1.num_batches_tracked', 'model.x8.0.3.conv_list.0.conv.weight', 'model.x8.0.3.conv_list.0.bn.weight', 'model.x8.0.3.conv_list.0.bn.bias', 'model.x8.0.3.conv_list.0.bn.running_mean', 'model.x8.0.3.conv_list.0.bn.running_var', 'model.x8.0.3.conv_list.0.bn.num_batches_tracked', 'model.x8.0.3.conv_list.1.conv.weight', 'model.x8.0.3.conv_list.1.bn.weight', 'model.x8.0.3.conv_list.1.bn.bias', 'model.x8.0.3.conv_list.1.bn.running_mean', 'model.x8.0.3.conv_list.1.bn.running_var', 'model.x8.0.3.conv_list.1.bn.num_batches_tracked', 'model.x8.0.3.conv_list.2.conv.weight', 'model.x8.0.3.conv_list.2.bn.weight', 'model.x8.0.3.conv_list.2.bn.bias', 'model.x8.0.3.conv_list.2.bn.running_mean', 'model.x8.0.3.conv_list.2.bn.running_var', 'model.x8.0.3.conv_list.2.bn.num_batches_tracked', 'model.x8.0.3.conv_list.3.conv.weight', 'model.x8.0.3.conv_list.3.bn.weight', 'model.x8.0.3.conv_list.3.bn.bias', 'model.x8.0.3.conv_list.3.bn.running_mean', 'model.x8.0.3.conv_list.3.bn.running_var', 'model.x8.0.3.conv_list.3.bn.num_batches_tracked', 'model.x8.0.4.conv_list.0.conv.weight', 'model.x8.0.4.conv_list.0.bn.weight', 'model.x8.0.4.conv_list.0.bn.bias', 'model.x8.0.4.conv_list.0.bn.running_mean', 'model.x8.0.4.conv_list.0.bn.running_var', 'model.x8.0.4.conv_list.0.bn.num_batches_tracked', 'model.x8.0.4.conv_list.1.conv.weight', 'model.x8.0.4.conv_list.1.bn.weight', 'model.x8.0.4.conv_list.1.bn.bias', 'model.x8.0.4.conv_list.1.bn.running_mean', 'model.x8.0.4.conv_list.1.bn.running_var', 'model.x8.0.4.conv_list.1.bn.num_batches_tracked', 'model.x8.0.4.conv_list.2.conv.weight', 'model.x8.0.4.conv_list.2.bn.weight', 'model.x8.0.4.conv_list.2.bn.bias', 'model.x8.0.4.conv_list.2.bn.running_mean', 'model.x8.0.4.conv_list.2.bn.running_var', 'model.x8.0.4.conv_list.2.bn.num_batches_tracked', 'model.x8.0.4.conv_list.3.conv.weight', 'model.x8.0.4.conv_list.3.bn.weight', 'model.x8.0.4.conv_list.3.bn.bias', 'model.x8.0.4.conv_list.3.bn.running_mean', 'model.x8.0.4.conv_list.3.bn.running_var', 'model.x8.0.4.conv_list.3.bn.num_batches_tracked', 'model.x8.0.5.conv_list.0.conv.weight', 'model.x8.0.5.conv_list.0.bn.weight', 'model.x8.0.5.conv_list.0.bn.bias', 'model.x8.0.5.conv_list.0.bn.running_mean', 'model.x8.0.5.conv_list.0.bn.running_var', 'model.x8.0.5.conv_list.0.bn.num_batches_tracked', 'model.x8.0.5.conv_list.1.conv.weight', 'model.x8.0.5.conv_list.1.bn.weight', 'model.x8.0.5.conv_list.1.bn.bias', 'model.x8.0.5.conv_list.1.bn.running_mean', 'model.x8.0.5.conv_list.1.bn.running_var', 'model.x8.0.5.conv_list.1.bn.num_batches_tracked', 'model.x8.0.5.conv_list.2.conv.weight', 'model.x8.0.5.conv_list.2.bn.weight', 'model.x8.0.5.conv_list.2.bn.bias', 'model.x8.0.5.conv_list.2.bn.running_mean', 'model.x8.0.5.conv_list.2.bn.running_var', 'model.x8.0.5.conv_list.2.bn.num_batches_tracked', 'model.x8.0.5.conv_list.3.conv.weight', 'model.x8.0.5.conv_list.3.bn.weight', 'model.x8.0.5.conv_list.3.bn.bias', 'model.x8.0.5.conv_list.3.bn.running_mean', 'model.x8.0.5.conv_list.3.bn.running_var', 'model.x8.0.5.conv_list.3.bn.num_batches_tracked', 'model.x16.0.6.conv_list.0.conv.weight', 'model.x16.0.6.conv_list.0.bn.weight', 'model.x16.0.6.conv_list.0.bn.bias', 'model.x16.0.6.conv_list.0.bn.running_mean', 'model.x16.0.6.conv_list.0.bn.running_var', 'model.x16.0.6.conv_list.0.bn.num_batches_tracked', 'model.x16.0.6.conv_list.1.conv.weight', 'model.x16.0.6.conv_list.1.bn.weight', 'model.x16.0.6.conv_list.1.bn.bias', 'model.x16.0.6.conv_list.1.bn.running_mean', 'model.x16.0.6.conv_list.1.bn.running_var', 'model.x16.0.6.conv_list.1.bn.num_batches_tracked', 'model.x16.0.6.conv_list.2.conv.weight', 'model.x16.0.6.conv_list.2.bn.weight', 'model.x16.0.6.conv_list.2.bn.bias', 'model.x16.0.6.conv_list.2.bn.running_mean', 'model.x16.0.6.conv_list.2.bn.running_var', 'model.x16.0.6.conv_list.2.bn.num_batches_tracked', 'model.x16.0.6.conv_list.3.conv.weight', 'model.x16.0.6.conv_list.3.bn.weight', 'model.x16.0.6.conv_list.3.bn.bias', 'model.x16.0.6.conv_list.3.bn.running_mean', 'model.x16.0.6.conv_list.3.bn.running_var', 'model.x16.0.6.conv_list.3.bn.num_batches_tracked', 'model.x16.0.6.avd_layer.0.weight', 'model.x16.0.6.avd_layer.1.weight', 'model.x16.0.6.avd_layer.1.bias', 'model.x16.0.6.avd_layer.1.running_mean', 'model.x16.0.6.avd_layer.1.running_var', 'model.x16.0.6.avd_layer.1.num_batches_tracked', 'model.x16.0.7.conv_list.0.conv.weight', 'model.x16.0.7.conv_list.0.bn.weight', 'model.x16.0.7.conv_list.0.bn.bias', 'model.x16.0.7.conv_list.0.bn.running_mean', 'model.x16.0.7.conv_list.0.bn.running_var', 'model.x16.0.7.conv_list.0.bn.num_batches_tracked', 'model.x16.0.7.conv_list.1.conv.weight', 'model.x16.0.7.conv_list.1.bn.weight', 'model.x16.0.7.conv_list.1.bn.bias', 'model.x16.0.7.conv_list.1.bn.running_mean', 'model.x16.0.7.conv_list.1.bn.running_var', 'model.x16.0.7.conv_list.1.bn.num_batches_tracked', 'model.x16.0.7.conv_list.2.conv.weight', 'model.x16.0.7.conv_list.2.bn.weight', 'model.x16.0.7.conv_list.2.bn.bias', 'model.x16.0.7.conv_list.2.bn.running_mean', 'model.x16.0.7.conv_list.2.bn.running_var', 'model.x16.0.7.conv_list.2.bn.num_batches_tracked', 'model.x16.0.7.conv_list.3.conv.weight', 'model.x16.0.7.conv_list.3.bn.weight', 'model.x16.0.7.conv_list.3.bn.bias', 'model.x16.0.7.conv_list.3.bn.running_mean', 'model.x16.0.7.conv_list.3.bn.running_var', 'model.x16.0.7.conv_list.3.bn.num_batches_tracked', 'model.x16.0.8.conv_list.0.conv.weight', 'model.x16.0.8.conv_list.0.bn.weight', 'model.x16.0.8.conv_list.0.bn.bias', 'model.x16.0.8.conv_list.0.bn.running_mean', 'model.x16.0.8.conv_list.0.bn.running_var', 'model.x16.0.8.conv_list.0.bn.num_batches_tracked', 'model.x16.0.8.conv_list.1.conv.weight', 'model.x16.0.8.conv_list.1.bn.weight', 'model.x16.0.8.conv_list.1.bn.bias', 'model.x16.0.8.conv_list.1.bn.running_mean', 'model.x16.0.8.conv_list.1.bn.running_var', 'model.x16.0.8.conv_list.1.bn.num_batches_tracked', 'model.x16.0.8.conv_list.2.conv.weight', 'model.x16.0.8.conv_list.2.bn.weight', 'model.x16.0.8.conv_list.2.bn.bias', 'model.x16.0.8.conv_list.2.bn.running_mean', 'model.x16.0.8.conv_list.2.bn.running_var', 'model.x16.0.8.conv_list.2.bn.num_batches_tracked', 'model.x16.0.8.conv_list.3.conv.weight', 'model.x16.0.8.conv_list.3.bn.weight', 'model.x16.0.8.conv_list.3.bn.bias', 'model.x16.0.8.conv_list.3.bn.running_mean', 'model.x16.0.8.conv_list.3.bn.running_var', 'model.x16.0.8.conv_list.3.bn.num_batches_tracked', 'model.x16.0.9.conv_list.0.conv.weight', 'model.x16.0.9.conv_list.0.bn.weight', 'model.x16.0.9.conv_list.0.bn.bias', 'model.x16.0.9.conv_list.0.bn.running_mean', 'model.x16.0.9.conv_list.0.bn.running_var', 'model.x16.0.9.conv_list.0.bn.num_batches_tracked', 'model.x16.0.9.conv_list.1.conv.weight', 'model.x16.0.9.conv_list.1.bn.weight', 'model.x16.0.9.conv_list.1.bn.bias', 'model.x16.0.9.conv_list.1.bn.running_mean', 'model.x16.0.9.conv_list.1.bn.running_var', 'model.x16.0.9.conv_list.1.bn.num_batches_tracked', 'model.x16.0.9.conv_list.2.conv.weight', 'model.x16.0.9.conv_list.2.bn.weight', 'model.x16.0.9.conv_list.2.bn.bias', 'model.x16.0.9.conv_list.2.bn.running_mean', 'model.x16.0.9.conv_list.2.bn.running_var', 'model.x16.0.9.conv_list.2.bn.num_batches_tracked', 'model.x16.0.9.conv_list.3.conv.weight', 'model.x16.0.9.conv_list.3.bn.weight', 'model.x16.0.9.conv_list.3.bn.bias', 'model.x16.0.9.conv_list.3.bn.running_mean', 'model.x16.0.9.conv_list.3.bn.running_var', 'model.x16.0.9.conv_list.3.bn.num_batches_tracked', 'model.x16.0.10.conv_list.0.conv.weight', 'model.x16.0.10.conv_list.0.bn.weight', 'model.x16.0.10.conv_list.0.bn.bias', 'model.x16.0.10.conv_list.0.bn.running_mean', 'model.x16.0.10.conv_list.0.bn.running_var', 'model.x16.0.10.conv_list.0.bn.num_batches_tracked', 'model.x16.0.10.conv_list.1.conv.weight', 'model.x16.0.10.conv_list.1.bn.weight', 'model.x16.0.10.conv_list.1.bn.bias', 'model.x16.0.10.conv_list.1.bn.running_mean', 'model.x16.0.10.conv_list.1.bn.running_var', 'model.x16.0.10.conv_list.1.bn.num_batches_tracked', 'model.x16.0.10.conv_list.2.conv.weight', 'model.x16.0.10.conv_list.2.bn.weight', 'model.x16.0.10.conv_list.2.bn.bias', 'model.x16.0.10.conv_list.2.bn.running_mean', 'model.x16.0.10.conv_list.2.bn.running_var', 'model.x16.0.10.conv_list.2.bn.num_batches_tracked', 'model.x16.0.10.conv_list.3.conv.weight', 'model.x16.0.10.conv_list.3.bn.weight', 'model.x16.0.10.conv_list.3.bn.bias', 'model.x16.0.10.conv_list.3.bn.running_mean', 'model.x16.0.10.conv_list.3.bn.running_var', 'model.x16.0.10.conv_list.3.bn.num_batches_tracked', 'model.x32.0.11.conv_list.0.conv.weight', 'model.x32.0.11.conv_list.0.bn.weight', 'model.x32.0.11.conv_list.0.bn.bias', 'model.x32.0.11.conv_list.0.bn.running_mean', 'model.x32.0.11.conv_list.0.bn.running_var', 'model.x32.0.11.conv_list.0.bn.num_batches_tracked', 'model.x32.0.11.conv_list.1.conv.weight', 'model.x32.0.11.conv_list.1.bn.weight', 'model.x32.0.11.conv_list.1.bn.bias', 'model.x32.0.11.conv_list.1.bn.running_mean', 'model.x32.0.11.conv_list.1.bn.running_var', 'model.x32.0.11.conv_list.1.bn.num_batches_tracked', 'model.x32.0.11.conv_list.2.conv.weight', 'model.x32.0.11.conv_list.2.bn.weight', 'model.x32.0.11.conv_list.2.bn.bias', 'model.x32.0.11.conv_list.2.bn.running_mean', 'model.x32.0.11.conv_list.2.bn.running_var', 'model.x32.0.11.conv_list.2.bn.num_batches_tracked', 'model.x32.0.11.conv_list.3.conv.weight', 'model.x32.0.11.conv_list.3.bn.weight', 'model.x32.0.11.conv_list.3.bn.bias', 'model.x32.0.11.conv_list.3.bn.running_mean', 'model.x32.0.11.conv_list.3.bn.running_var', 'model.x32.0.11.conv_list.3.bn.num_batches_tracked', 'model.x32.0.11.avd_layer.0.weight', 'model.x32.0.11.avd_layer.1.weight', 'model.x32.0.11.avd_layer.1.bias', 'model.x32.0.11.avd_layer.1.running_mean', 'model.x32.0.11.avd_layer.1.running_var', 'model.x32.0.11.avd_layer.1.num_batches_tracked', 'model.x32.0.12.conv_list.0.conv.weight', 'model.x32.0.12.conv_list.0.bn.weight', 'model.x32.0.12.conv_list.0.bn.bias', 'model.x32.0.12.conv_list.0.bn.running_mean', 'model.x32.0.12.conv_list.0.bn.running_var', 'model.x32.0.12.conv_list.0.bn.num_batches_tracked', 'model.x32.0.12.conv_list.1.conv.weight', 'model.x32.0.12.conv_list.1.bn.weight', 'model.x32.0.12.conv_list.1.bn.bias', 'model.x32.0.12.conv_list.1.bn.running_mean', 'model.x32.0.12.conv_list.1.bn.running_var', 'model.x32.0.12.conv_list.1.bn.num_batches_tracked', 'model.x32.0.12.conv_list.2.conv.weight', 'model.x32.0.12.conv_list.2.bn.weight', 'model.x32.0.12.conv_list.2.bn.bias', 'model.x32.0.12.conv_list.2.bn.running_mean', 'model.x32.0.12.conv_list.2.bn.running_var', 'model.x32.0.12.conv_list.2.bn.num_batches_tracked', 'model.x32.0.12.conv_list.3.conv.weight', 'model.x32.0.12.conv_list.3.bn.weight', 'model.x32.0.12.conv_list.3.bn.bias', 'model.x32.0.12.conv_list.3.bn.running_mean', 'model.x32.0.12.conv_list.3.bn.running_var', 'model.x32.0.12.conv_list.3.bn.num_batches_tracked', 'model.x32.0.13.conv_list.0.conv.weight', 'model.x32.0.13.conv_list.0.bn.weight', 'model.x32.0.13.conv_list.0.bn.bias', 'model.x32.0.13.conv_list.0.bn.running_mean', 'model.x32.0.13.conv_list.0.bn.running_var', 'model.x32.0.13.conv_list.0.bn.num_batches_tracked', 'model.x32.0.13.conv_list.1.conv.weight', 'model.x32.0.13.conv_list.1.bn.weight', 'model.x32.0.13.conv_list.1.bn.bias', 'model.x32.0.13.conv_list.1.bn.running_mean', 'model.x32.0.13.conv_list.1.bn.running_var', 'model.x32.0.13.conv_list.1.bn.num_batches_tracked', 'model.x32.0.13.conv_list.2.conv.weight', 'model.x32.0.13.conv_list.2.bn.weight', 'model.x32.0.13.conv_list.2.bn.bias', 'model.x32.0.13.conv_list.2.bn.running_mean', 'model.x32.0.13.conv_list.2.bn.running_var', 'model.x32.0.13.conv_list.2.bn.num_batches_tracked', 'model.x32.0.13.conv_list.3.conv.weight', 'model.x32.0.13.conv_list.3.bn.weight', 'model.x32.0.13.conv_list.3.bn.bias', 'model.x32.0.13.conv_list.3.bn.running_mean', 'model.x32.0.13.conv_list.3.bn.running_var', 'model.x32.0.13.conv_list.3.bn.num_batches_tracked'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 1024)\n",
    "    # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# GTA5_dataset = GTA5(GTA5_path=GTA5_PATH, transform=transform)\n",
    "\n",
    "\n",
    "def load_student_checkpoint(checkpoint_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load and process student model checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed state dict containing only student model weights\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    state_dict = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Extract and process student weights\n",
    "    student_state_dict = {k.replace('student.model.', ''): v \n",
    "                         for k, v in state_dict['state_dict'].items() \n",
    "                         if k.startswith('student') and not k.startswith('student.feature_matchers')}\n",
    "    \n",
    "    return student_state_dict\n",
    "\n",
    "# Load checkpoint and save state dict\n",
    "checkpoint_path = '/home/arda/dinov2/distillation/logs/stdc/distillation/version_7/checkpoints/epoch=29-val_similarity=0.36.ckpt'\n",
    "student_state_dict = load_student_checkpoint(checkpoint_path)\n",
    "torch.save(student_state_dict, '/home/arda/dinov2/distillation/logs/stdc/distillation/version_7/checkpoints/student_state_dict.pth')\n",
    "student_state_dict.keys()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.num_batches_tracked', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load pretrained ResNet50 from torchvision\n",
    "# import torchvision.models as models\n",
    "# resnet50 = models.resnet50(pretrained=True)\n",
    "# resnet50.to(device)\n",
    "# resnet50.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create mapping between torchvision ResNet50 and student model keys\n",
    "# key_mapping = {\n",
    "#     # Stem mapping\n",
    "#     'conv1.weight': 'model.stem.conv1.weight',\n",
    "#     'bn1.weight': 'model.stem.conv1.norm.weight',\n",
    "#     'bn1.bias': 'model.stem.conv1.norm.bias',\n",
    "#     'bn1.running_mean': 'model.stem.conv1.norm.running_mean', \n",
    "#     'bn1.running_var': 'model.stem.conv1.norm.running_var',\n",
    "#     'bn1.num_batches_tracked': 'model.stem.conv1.norm.num_batches_tracked',\n",
    "# }\n",
    "\n",
    "# # Add mappings for each layer\n",
    "# for i in range(1, 5):  # ResNet50 has 4 layers\n",
    "#     for j in range(3):  # Each layer has 3 bottleneck blocks\n",
    "#         # Map each component of the bottleneck block\n",
    "#         for k in range(1, 4):  # Each block has 3 conv layers\n",
    "#             torchvision_prefix = f'layer{i}.{j}'\n",
    "#             student_prefix = f'model.res{i+1}.{j}'\n",
    "            \n",
    "#             # Conv layers\n",
    "#             key_mapping[f'{torchvision_prefix}.conv{k}.weight'] = f'{student_prefix}.conv{k}.weight'\n",
    "            \n",
    "#             # Batch norm layers\n",
    "#             key_mapping[f'{torchvision_prefix}.bn{k}.weight'] = f'{student_prefix}.conv{k}.norm.weight'\n",
    "#             key_mapping[f'{torchvision_prefix}.bn{k}.bias'] = f'{student_prefix}.conv{k}.norm.bias'\n",
    "#             key_mapping[f'{torchvision_prefix}.bn{k}.running_mean'] = f'{student_prefix}.conv{k}.norm.running_mean'\n",
    "#             key_mapping[f'{torchvision_prefix}.bn{k}.running_var'] = f'{student_prefix}.conv{k}.norm.running_var'\n",
    "#             key_mapping[f'{torchvision_prefix}.bn{k}.num_batches_tracked'] = f'{student_prefix}.conv{k}.norm.num_batches_tracked'\n",
    "            \n",
    "#         # Map downsample layers if they exist\n",
    "#         if j == 0:  # Only first block in each layer has downsample\n",
    "#             key_mapping[f'layer{i}.0.downsample.0.weight'] = f'model.res{i+1}.0.shortcut.weight'\n",
    "#             key_mapping[f'layer{i}.0.downsample.1.weight'] = f'model.res{i+1}.0.shortcut.norm.weight'\n",
    "#             key_mapping[f'layer{i}.0.downsample.1.bias'] = f'model.res{i+1}.0.shortcut.norm.bias'\n",
    "#             key_mapping[f'layer{i}.0.downsample.1.running_mean'] = f'model.res{i+1}.0.shortcut.norm.running_mean'\n",
    "#             key_mapping[f'layer{i}.0.downsample.1.running_var'] = f'model.res{i+1}.0.shortcut.norm.running_var'\n",
    "#             key_mapping[f'layer{i}.0.downsample.1.num_batches_tracked'] = f'model.res{i+1}.0.shortcut.norm.num_batches_tracked'\n",
    "\n",
    "# # Create new state dict with mapped keys\n",
    "# mapped_state_dict = {}\n",
    "# for k, v in resnet50.state_dict().items():\n",
    "#     if k in key_mapping:\n",
    "#         mapped_state_dict[key_mapping[k]] = v\n",
    "\n",
    "# # Load the mapped state dict into student model\n",
    "# student_state_dict.update(mapped_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.resnet_wrapper import ResNetWrapper\n",
    "encoder = ResNetWrapper(depth=50, out_features=['res5'])\n",
    "encoder.load_state_dict(student_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder.eval()\n",
    "encoder.to(device)\n",
    "encoder.model.eval()\n",
    "encoder = encoder.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = torch.randn(1, 3, 512, 1024).to(device)\n",
    "encoder(asd)[\"res5\"].shape\n",
    "\n",
    "# Freeze all parameters of the encoder\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [04:58<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Loss: 0.8306\n",
      "Pixel Accuracy: 0.8372\n",
      "Mean Class Accuracy: 0.3153\n",
      "Mean IoU: 0.2264\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9398, IoU: 0.9225\n",
      "Class  1 - Acc: 0.5606, IoU: 0.3499\n",
      "Class  2 - Acc: 0.6936, IoU: 0.6392\n",
      "Class  3 - Acc: 0.4527, IoU: 0.1909\n",
      "Class  4 - Acc: 0.0188, IoU: 0.0001\n",
      "Class  5 - Acc: 0.0234, IoU: 0.0015\n",
      "Class  6 - Acc: 0.0036, IoU: 0.0001\n",
      "Class  7 - Acc: 0.0013, IoU: 0.0000\n",
      "Class  8 - Acc: 0.6751, IoU: 0.5282\n",
      "Class  9 - Acc: 0.6168, IoU: 0.3076\n",
      "Class 10 - Acc: 0.9087, IoU: 0.8485\n",
      "Class 11 - Acc: 0.0066, IoU: 0.0005\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.6739, IoU: 0.4691\n",
      "Class 14 - Acc: 0.3938, IoU: 0.0370\n",
      "Class 15 - Acc: 0.0041, IoU: 0.0009\n",
      "Class 16 - Acc: 0.0168, IoU: 0.0059\n",
      "Class 17 - Acc: 0.0007, IoU: 0.0004\n",
      "Class 18 - Acc: 0.0001, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [05:03<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10\n",
      "Loss: 0.4657\n",
      "Pixel Accuracy: 0.8774\n",
      "Mean Class Accuracy: 0.4680\n",
      "Mean IoU: 0.2936\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9635, IoU: 0.9439\n",
      "Class  1 - Acc: 0.6833, IoU: 0.5072\n",
      "Class  2 - Acc: 0.7679, IoU: 0.7086\n",
      "Class  3 - Acc: 0.5749, IoU: 0.3549\n",
      "Class  4 - Acc: 0.9668, IoU: 0.0000\n",
      "Class  5 - Acc: 0.5494, IoU: 0.0057\n",
      "Class  6 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  7 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  8 - Acc: 0.7590, IoU: 0.6195\n",
      "Class  9 - Acc: 0.7045, IoU: 0.4892\n",
      "Class 10 - Acc: 0.9244, IoU: 0.8840\n",
      "Class 11 - Acc: 0.2000, IoU: 0.0000\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.7732, IoU: 0.6677\n",
      "Class 14 - Acc: 0.5850, IoU: 0.3976\n",
      "Class 15 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 16 - Acc: 0.4403, IoU: 0.0000\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [05:00<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10\n",
      "Loss: 0.3906\n",
      "Pixel Accuracy: 0.8857\n",
      "Mean Class Accuracy: 0.4597\n",
      "Mean IoU: 0.3160\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9659, IoU: 0.9470\n",
      "Class  1 - Acc: 0.7114, IoU: 0.5351\n",
      "Class  2 - Acc: 0.7877, IoU: 0.7262\n",
      "Class  3 - Acc: 0.6198, IoU: 0.4046\n",
      "Class  4 - Acc: 0.5523, IoU: 0.0540\n",
      "Class  5 - Acc: 0.4975, IoU: 0.1100\n",
      "Class  6 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  7 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  8 - Acc: 0.7734, IoU: 0.6386\n",
      "Class  9 - Acc: 0.7307, IoU: 0.5263\n",
      "Class 10 - Acc: 0.9300, IoU: 0.8905\n",
      "Class 11 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8035, IoU: 0.7005\n",
      "Class 14 - Acc: 0.6140, IoU: 0.4585\n",
      "Class 15 - Acc: 0.0672, IoU: 0.0000\n",
      "Class 16 - Acc: 0.6818, IoU: 0.0125\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [04:55<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n",
      "Loss: 0.3518\n",
      "Pixel Accuracy: 0.8925\n",
      "Mean Class Accuracy: 0.5109\n",
      "Mean IoU: 0.3442\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9690, IoU: 0.9512\n",
      "Class  1 - Acc: 0.7336, IoU: 0.5620\n",
      "Class  2 - Acc: 0.8054, IoU: 0.7417\n",
      "Class  3 - Acc: 0.6490, IoU: 0.4280\n",
      "Class  4 - Acc: 0.5472, IoU: 0.1737\n",
      "Class  5 - Acc: 0.5167, IoU: 0.1548\n",
      "Class  6 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  7 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  8 - Acc: 0.7852, IoU: 0.6528\n",
      "Class  9 - Acc: 0.7471, IoU: 0.5519\n",
      "Class 10 - Acc: 0.9345, IoU: 0.8961\n",
      "Class 11 - Acc: 0.5065, IoU: 0.0001\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8144, IoU: 0.7139\n",
      "Class 14 - Acc: 0.6393, IoU: 0.4890\n",
      "Class 15 - Acc: 0.4503, IoU: 0.0673\n",
      "Class 16 - Acc: 0.6088, IoU: 0.1571\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [04:59<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n",
      "Loss: 0.3237\n",
      "Pixel Accuracy: 0.8986\n",
      "Mean Class Accuracy: 0.5695\n",
      "Mean IoU: 0.3784\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9710, IoU: 0.9537\n",
      "Class  1 - Acc: 0.7448, IoU: 0.5791\n",
      "Class  2 - Acc: 0.8206, IoU: 0.7554\n",
      "Class  3 - Acc: 0.6778, IoU: 0.4612\n",
      "Class  4 - Acc: 0.5469, IoU: 0.2201\n",
      "Class  5 - Acc: 0.5318, IoU: 0.1763\n",
      "Class  6 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  7 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  8 - Acc: 0.7899, IoU: 0.6616\n",
      "Class  9 - Acc: 0.7613, IoU: 0.5733\n",
      "Class 10 - Acc: 0.9374, IoU: 0.8996\n",
      "Class 11 - Acc: 0.1944, IoU: 0.0009\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8268, IoU: 0.7332\n",
      "Class 14 - Acc: 0.7216, IoU: 0.5451\n",
      "Class 15 - Acc: 0.6491, IoU: 0.2787\n",
      "Class 16 - Acc: 0.6472, IoU: 0.3516\n",
      "Class 17 - Acc: 1.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [04:59<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n",
      "Loss: 0.3063\n",
      "Pixel Accuracy: 0.9027\n",
      "Mean Class Accuracy: 0.5891\n",
      "Mean IoU: 0.3990\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9716, IoU: 0.9551\n",
      "Class  1 - Acc: 0.7551, IoU: 0.5898\n",
      "Class  2 - Acc: 0.8305, IoU: 0.7660\n",
      "Class  3 - Acc: 0.6914, IoU: 0.4779\n",
      "Class  4 - Acc: 0.5610, IoU: 0.2469\n",
      "Class  5 - Acc: 0.5531, IoU: 0.1968\n",
      "Class  6 - Acc: 1.0000, IoU: 0.0000\n",
      "Class  7 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  8 - Acc: 0.7947, IoU: 0.6687\n",
      "Class  9 - Acc: 0.7708, IoU: 0.5850\n",
      "Class 10 - Acc: 0.9400, IoU: 0.9032\n",
      "Class 11 - Acc: 0.3188, IoU: 0.0224\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8326, IoU: 0.7395\n",
      "Class 14 - Acc: 0.7593, IoU: 0.5778\n",
      "Class 15 - Acc: 0.6971, IoU: 0.3741\n",
      "Class 16 - Acc: 0.7175, IoU: 0.4774\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [05:00<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n",
      "Loss: 0.2879\n",
      "Pixel Accuracy: 0.9070\n",
      "Mean Class Accuracy: 0.6551\n",
      "Mean IoU: 0.4237\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9733, IoU: 0.9575\n",
      "Class  1 - Acc: 0.7669, IoU: 0.6053\n",
      "Class  2 - Acc: 0.8379, IoU: 0.7734\n",
      "Class  3 - Acc: 0.7093, IoU: 0.4956\n",
      "Class  4 - Acc: 0.5746, IoU: 0.2726\n",
      "Class  5 - Acc: 0.5684, IoU: 0.2110\n",
      "Class  6 - Acc: 0.6714, IoU: 0.0077\n",
      "Class  7 - Acc: 0.9047, IoU: 0.0168\n",
      "Class  8 - Acc: 0.8027, IoU: 0.6777\n",
      "Class  9 - Acc: 0.7799, IoU: 0.6029\n",
      "Class 10 - Acc: 0.9416, IoU: 0.9055\n",
      "Class 11 - Acc: 0.4485, IoU: 0.1534\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8461, IoU: 0.7616\n",
      "Class 14 - Acc: 0.7860, IoU: 0.6102\n",
      "Class 15 - Acc: 0.7789, IoU: 0.4482\n",
      "Class 16 - Acc: 0.7469, IoU: 0.5513\n",
      "Class 17 - Acc: 0.3095, IoU: 0.0001\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [04:57<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n",
      "Loss: 0.2735\n",
      "Pixel Accuracy: 0.9108\n",
      "Mean Class Accuracy: 0.6755\n",
      "Mean IoU: 0.4583\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9748, IoU: 0.9592\n",
      "Class  1 - Acc: 0.7771, IoU: 0.6197\n",
      "Class  2 - Acc: 0.8456, IoU: 0.7819\n",
      "Class  3 - Acc: 0.7225, IoU: 0.5151\n",
      "Class  4 - Acc: 0.5906, IoU: 0.2929\n",
      "Class  5 - Acc: 0.5847, IoU: 0.2260\n",
      "Class  6 - Acc: 0.5912, IoU: 0.0709\n",
      "Class  7 - Acc: 0.8288, IoU: 0.2413\n",
      "Class  8 - Acc: 0.8088, IoU: 0.6859\n",
      "Class  9 - Acc: 0.7898, IoU: 0.6169\n",
      "Class 10 - Acc: 0.9441, IoU: 0.9087\n",
      "Class 11 - Acc: 0.5317, IoU: 0.2718\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8533, IoU: 0.7699\n",
      "Class 14 - Acc: 0.7997, IoU: 0.6310\n",
      "Class 15 - Acc: 0.7623, IoU: 0.4514\n",
      "Class 16 - Acc: 0.7732, IoU: 0.5798\n",
      "Class 17 - Acc: 0.6573, IoU: 0.0859\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [04:59<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n",
      "Loss: 0.2584\n",
      "Pixel Accuracy: 0.9147\n",
      "Mean Class Accuracy: 0.6825\n",
      "Mean IoU: 0.4991\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9769, IoU: 0.9619\n",
      "Class  1 - Acc: 0.7865, IoU: 0.6363\n",
      "Class  2 - Acc: 0.8521, IoU: 0.7884\n",
      "Class  3 - Acc: 0.7363, IoU: 0.5310\n",
      "Class  4 - Acc: 0.6043, IoU: 0.3181\n",
      "Class  5 - Acc: 0.5987, IoU: 0.2400\n",
      "Class  6 - Acc: 0.5599, IoU: 0.1105\n",
      "Class  7 - Acc: 0.7606, IoU: 0.3243\n",
      "Class  8 - Acc: 0.8134, IoU: 0.6918\n",
      "Class  9 - Acc: 0.7952, IoU: 0.6286\n",
      "Class 10 - Acc: 0.9457, IoU: 0.9105\n",
      "Class 11 - Acc: 0.5703, IoU: 0.3089\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8619, IoU: 0.7828\n",
      "Class 14 - Acc: 0.8351, IoU: 0.6770\n",
      "Class 15 - Acc: 0.8378, IoU: 0.5885\n",
      "Class 16 - Acc: 0.8224, IoU: 0.6386\n",
      "Class 17 - Acc: 0.6096, IoU: 0.3460\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [05:03<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n",
      "Loss: 0.2473\n",
      "Pixel Accuracy: 0.9175\n",
      "Mean Class Accuracy: 0.7309\n",
      "Mean IoU: 0.5156\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9777, IoU: 0.9631\n",
      "Class  1 - Acc: 0.7910, IoU: 0.6440\n",
      "Class  2 - Acc: 0.8601, IoU: 0.7979\n",
      "Class  3 - Acc: 0.7574, IoU: 0.5576\n",
      "Class  4 - Acc: 0.6202, IoU: 0.3400\n",
      "Class  5 - Acc: 0.6131, IoU: 0.2537\n",
      "Class  6 - Acc: 0.5646, IoU: 0.1300\n",
      "Class  7 - Acc: 0.7487, IoU: 0.3514\n",
      "Class  8 - Acc: 0.8163, IoU: 0.6964\n",
      "Class  9 - Acc: 0.7987, IoU: 0.6347\n",
      "Class 10 - Acc: 0.9471, IoU: 0.9122\n",
      "Class 11 - Acc: 0.5891, IoU: 0.3240\n",
      "Class 12 - Acc: 0.8488, IoU: 0.0045\n",
      "Class 13 - Acc: 0.8644, IoU: 0.7846\n",
      "Class 14 - Acc: 0.8366, IoU: 0.6859\n",
      "Class 15 - Acc: 0.8333, IoU: 0.5910\n",
      "Class 16 - Acc: 0.8482, IoU: 0.6902\n",
      "Class 17 - Acc: 0.5716, IoU: 0.4358\n",
      "Class 18 - Acc: 0.0000, IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a simple decoder network\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "class SegmentationDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels=2048, num_classes=19):\n",
    "        super().__init__()\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            # 16x32 -> 32x64\n",
    "            torch.nn.ConvTranspose2d(in_channels, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(1024),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 32x64 -> 64x128\n",
    "            torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 64x128 -> 128x256\n",
    "            torch.nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 128x256 -> 256x512\n",
    "            torch.nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 256x512 -> 512x1024\n",
    "            torch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Final 1x1 conv to get to num_classes\n",
    "            torch.nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        # Ensure exact output size\n",
    "        if x.shape[-2:] != (512, 1024):\n",
    "            x = torch.nn.functional.interpolate(\n",
    "                x, size=(512, 1024), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "        return x\n",
    "\n",
    "# Initialize decoder, optimizer, and loss function\n",
    "decoder = SegmentationDecoder().to(device)\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "def fast_hist(a: np.ndarray, b: np.ndarray, n: int) -> np.ndarray:\n",
    "    k = (b >= 0) & (b < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "def per_class_iou(hist: np.ndarray) -> np.ndarray:\n",
    "    epsilon = 1e-5\n",
    "    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
    "\n",
    "def train_epoch(encoder, decoder, dataloader, optimizer, criterion, device, num_classes=19):\n",
    "    decoder.train()\n",
    "    encoder.eval()  # Keep DINO frozen\n",
    "    \n",
    "    total_loss = 0\n",
    "    hist = np.zeros((num_classes, num_classes))  # Single histogram for entire epoch\n",
    "    total_pixels = 0\n",
    "    correct_pixels = 0\n",
    "    \n",
    "    for images, labels in tqdm.tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get DINO features\n",
    "        with torch.no_grad():\n",
    "            features = encoder(images)[\"res5\"]\n",
    "        \n",
    "        # Forward pass through decoder\n",
    "        outputs = decoder(features)\n",
    "        \n",
    "        # Resize outputs to match label size if needed\n",
    "        if outputs.shape[-2:] != labels.shape[-2:]:\n",
    "            outputs = torch.nn.functional.interpolate(\n",
    "                outputs, size=labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "        \n",
    "        # Pixel Accuracy\n",
    "        valid_mask = labels != 255  # Ignore index\n",
    "        total_pixels += valid_mask.sum().item()\n",
    "        correct_pixels += ((preds == labels) & valid_mask).sum().item()\n",
    "        \n",
    "        # IoU\n",
    "        preds = preds.cpu().numpy()\n",
    "        target = labels.cpu().numpy()\n",
    "        hist += fast_hist(preds.flatten(), target.flatten(), num_classes)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    pixel_acc = correct_pixels / total_pixels\n",
    "    \n",
    "    # Per-class accuracy (mean class accuracy)\n",
    "    class_acc = np.diag(hist) / (hist.sum(1) + np.finfo(np.float32).eps)\n",
    "    mean_class_acc = np.nanmean(class_acc)\n",
    "    \n",
    "    # IoU metrics\n",
    "    iou = per_class_iou(hist)\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'pixel_acc': pixel_acc,\n",
    "        'mean_class_acc': mean_class_acc,\n",
    "        'mean_iou': mean_iou,\n",
    "        'class_iou': iou,\n",
    "        'class_acc': class_acc\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    GTA5_dataset, \n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    metrics = train_epoch(encoder, decoder, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Loss: {metrics['loss']:.4f}\")\n",
    "    print(f\"Pixel Accuracy: {metrics['pixel_acc']:.4f}\")\n",
    "    print(f\"Mean Class Accuracy: {metrics['mean_class_acc']:.4f}\")\n",
    "    print(f\"Mean IoU: {metrics['mean_iou']:.4f}\")\n",
    "    \n",
    "    # Optionally print per-class metrics\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for i in range(19):  # Assuming 19 classes\n",
    "        print(f\"Class {i:2d} - Acc: {metrics['class_acc'][i]:.4f}, IoU: {metrics['class_iou'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 3/10\n",
    "# Loss: 1.2027\n",
    "# Pixel Accuracy: 0.6440\n",
    "# Mean Class Accuracy: 0.1213\n",
    "# Mean IoU: 0.0818\n",
    "\n",
    "# Per-class metrics:\n",
    "# Class  0 - Acc: 0.6736, IoU: 0.6670\n",
    "# Class  1 - Acc: 0.1000, IoU: 0.0000\n",
    "# Class  2 - Acc: 0.3823, IoU: 0.1999\n",
    "# Class  3 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class  4 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class  5 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class  6 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class  7 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class  8 - Acc: 0.4519, IoU: 0.0889\n",
    "# Class  9 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class 10 - Acc: 0.6970, IoU: 0.5976\n",
    "# Class 11 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class 12 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class 13 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class 14 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class 15 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class 16 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class 17 - Acc: 0.0000, IoU: 0.0000\n",
    "# Class 18 - Acc: 0.0000, IoU: 0.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_id_to_color' from 'datasets' (/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_id_to_color   \n\u001b[1;32m      5\u001b[0m img_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m310\u001b[39m\n\u001b[1;32m      7\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m encoder(GTA5_dataset[img_idx][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, reshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_class_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_id_to_color' from 'datasets' (/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import get_id_to_color   \n",
    "\n",
    "\n",
    "img_idx = 310\n",
    "\n",
    "embeddings = encoder(GTA5_dataset[img_idx][0].unsqueeze(0).to(device), n=1, reshape=True, return_class_token=False, norm=False)[0]\n",
    "out = decoder(embeddings)\n",
    "id_to_color = get_id_to_color()\n",
    "\n",
    "pred = out.argmax(1).cpu().numpy()\n",
    "pred = pred.reshape(518, 1036)\n",
    "# Convert class IDs to RGB colors\n",
    "color_map = np.array([id_to_color.get(i, (0, 0, 0)) for i in range(max(id_to_color.keys()) + 1)])\n",
    "pred_rgb = color_map[pred]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pred_rgb)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "labels = GTA5_dataset[img_idx][1].cpu().numpy()\n",
    "color_map = np.array([id_to_color.get(i, (0, 0, 0)) for i in range(max(id_to_color.keys()) + 1)])\n",
    "pred_rgb = np.zeros((*labels.shape, 3), dtype=np.uint8)\n",
    "mask = labels < len(color_map)\n",
    "pred_rgb[mask] = color_map[labels[mask]]\n",
    "plt.imshow(pred_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([518, 1036])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GTA5_dataset[img_idx][1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
