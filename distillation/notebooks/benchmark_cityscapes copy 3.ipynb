{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arda/miniconda3/envs/dinov2/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets_gta5 import GTA5, CityScapes\n",
    "import albumentations as A\n",
    "import torch\n",
    "\n",
    "CITYSCAPES_PATH = '/home/arda/.cache/kagglehub/datasets/ardaerendoru/gtagta/versions/1/Cityscapes/Cityscapes'\n",
    "\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 1024),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "CITYSCAPES_dataset = CityScapes(CITYSCAPES_PATH, train_val='train', transform=transform)\n",
    "\n",
    "\n",
    "def load_student_checkpoint(checkpoint_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load and process student model checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed state dict containing only student model weights\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    state_dict = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Extract and process student weights\n",
    "    student_state_dict = {k.replace('student.model.', ''): v \n",
    "                         for k, v in state_dict['state_dict'].items() \n",
    "                         if k.startswith('student') and not k.startswith('student.feature_matchers')}\n",
    "    \n",
    "    return student_state_dict\n",
    "\n",
    "# Load checkpoint and save state dict\n",
    "checkpoint_path = '/home/arda/dinov2/distillation/logs/resnet50/distillation/dino_s_full_scalekd/checkpoints/last.ckpt'\n",
    "student_state_dict = load_student_checkpoint(checkpoint_path)\n",
    "# torch.save(student_state_dict, '/home/arda/dinov2/distillation/logs/resnet50/distillation/version_7/checkpoints/student_state_dict.pth')\n",
    "# student_state_dict.keys()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/arda/dinov2/distillation')\n",
    "from models.resnet_wrapper import ResNetWrapper\n",
    "encoder = ResNetWrapper(depth=50, out_features=['res5'])\n",
    "encoder.load_state_dict(student_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder.eval()\n",
    "encoder.to(device)\n",
    "encoder.model.eval()\n",
    "encoder = encoder.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = torch.randn(1, 3, 512, 1024).to(device)\n",
    "encoder(asd)[\"res5\"].shape\n",
    "\n",
    "# # Freeze all parameters of the encoder\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:21<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Loss: 1.1662\n",
      "Pixel Accuracy: 0.8138\n",
      "Mean Class Accuracy: 0.2825\n",
      "Mean IoU: 0.2166\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9126, IoU: 0.8739\n",
      "Class  1 - Acc: 0.6781, IoU: 0.4225\n",
      "Class  2 - Acc: 0.8256, IoU: 0.7680\n",
      "Class  3 - Acc: 0.0460, IoU: 0.0006\n",
      "Class  4 - Acc: 0.0615, IoU: 0.0360\n",
      "Class  5 - Acc: 0.0305, IoU: 0.0016\n",
      "Class  6 - Acc: 0.0019, IoU: 0.0001\n",
      "Class  7 - Acc: 0.0081, IoU: 0.0037\n",
      "Class  8 - Acc: 0.7805, IoU: 0.6840\n",
      "Class  9 - Acc: 0.0116, IoU: 0.0000\n",
      "Class 10 - Acc: 0.8149, IoU: 0.5956\n",
      "Class 11 - Acc: 0.4612, IoU: 0.1397\n",
      "Class 12 - Acc: 0.0014, IoU: 0.0005\n",
      "Class 13 - Acc: 0.7020, IoU: 0.5830\n",
      "Class 14 - Acc: 0.0032, IoU: 0.0001\n",
      "Class 15 - Acc: 0.0021, IoU: 0.0013\n",
      "Class 16 - Acc: 0.0175, IoU: 0.0023\n",
      "Class 17 - Acc: 0.0012, IoU: 0.0003\n",
      "Class 18 - Acc: 0.0073, IoU: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:20<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10\n",
      "Loss: 0.5651\n",
      "Pixel Accuracy: 0.8816\n",
      "Mean Class Accuracy: 0.4976\n",
      "Mean IoU: 0.2904\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9592, IoU: 0.9304\n",
      "Class  1 - Acc: 0.7370, IoU: 0.6188\n",
      "Class  2 - Acc: 0.8608, IoU: 0.8168\n",
      "Class  3 - Acc: 0.0000, IoU: 0.0000\n",
      "Class  4 - Acc: 0.5119, IoU: 0.2124\n",
      "Class  5 - Acc: 0.2820, IoU: 0.0016\n",
      "Class  6 - Acc: 1.0000, IoU: 0.0000\n",
      "Class  7 - Acc: 0.7279, IoU: 0.0278\n",
      "Class  8 - Acc: 0.8605, IoU: 0.7992\n",
      "Class  9 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 10 - Acc: 0.8706, IoU: 0.8122\n",
      "Class 11 - Acc: 0.6467, IoU: 0.5119\n",
      "Class 12 - Acc: 0.1463, IoU: 0.0002\n",
      "Class 13 - Acc: 0.8343, IoU: 0.7837\n",
      "Class 14 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 15 - Acc: 0.0483, IoU: 0.0000\n",
      "Class 16 - Acc: 0.5000, IoU: 0.0000\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.4685, IoU: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:21<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10\n",
      "Loss: 0.4210\n",
      "Pixel Accuracy: 0.8960\n",
      "Mean Class Accuracy: 0.5986\n",
      "Mean IoU: 0.3554\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9645, IoU: 0.9386\n",
      "Class  1 - Acc: 0.7750, IoU: 0.6584\n",
      "Class  2 - Acc: 0.8789, IoU: 0.8361\n",
      "Class  3 - Acc: 0.6822, IoU: 0.0666\n",
      "Class  4 - Acc: 0.6243, IoU: 0.3690\n",
      "Class  5 - Acc: 0.5865, IoU: 0.0989\n",
      "Class  6 - Acc: 0.9640, IoU: 0.0003\n",
      "Class  7 - Acc: 0.7165, IoU: 0.2929\n",
      "Class  8 - Acc: 0.8781, IoU: 0.8194\n",
      "Class  9 - Acc: 0.7898, IoU: 0.1991\n",
      "Class 10 - Acc: 0.9085, IoU: 0.8502\n",
      "Class 11 - Acc: 0.6884, IoU: 0.5675\n",
      "Class 12 - Acc: 0.0370, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8581, IoU: 0.8112\n",
      "Class 14 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 15 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 16 - Acc: 0.4222, IoU: 0.0000\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.6002, IoU: 0.2435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:19<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n",
      "Loss: 0.3505\n",
      "Pixel Accuracy: 0.9069\n",
      "Mean Class Accuracy: 0.6120\n",
      "Mean IoU: 0.4123\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9670, IoU: 0.9431\n",
      "Class  1 - Acc: 0.8099, IoU: 0.6863\n",
      "Class  2 - Acc: 0.8968, IoU: 0.8517\n",
      "Class  3 - Acc: 0.6561, IoU: 0.2631\n",
      "Class  4 - Acc: 0.6630, IoU: 0.4198\n",
      "Class  5 - Acc: 0.5851, IoU: 0.2231\n",
      "Class  6 - Acc: 0.9374, IoU: 0.0008\n",
      "Class  7 - Acc: 0.7381, IoU: 0.3857\n",
      "Class  8 - Acc: 0.8985, IoU: 0.8363\n",
      "Class  9 - Acc: 0.7329, IoU: 0.4603\n",
      "Class 10 - Acc: 0.9302, IoU: 0.8741\n",
      "Class 11 - Acc: 0.7221, IoU: 0.6021\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8739, IoU: 0.8282\n",
      "Class 14 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 15 - Acc: 0.6207, IoU: 0.0000\n",
      "Class 16 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.5972, IoU: 0.4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:14<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n",
      "Loss: 0.3049\n",
      "Pixel Accuracy: 0.9139\n",
      "Mean Class Accuracy: 0.6498\n",
      "Mean IoU: 0.4356\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9705, IoU: 0.9483\n",
      "Class  1 - Acc: 0.8308, IoU: 0.7137\n",
      "Class  2 - Acc: 0.9082, IoU: 0.8635\n",
      "Class  3 - Acc: 0.6716, IoU: 0.3519\n",
      "Class  4 - Acc: 0.6955, IoU: 0.4612\n",
      "Class  5 - Acc: 0.5986, IoU: 0.2626\n",
      "Class  6 - Acc: 0.9005, IoU: 0.0574\n",
      "Class  7 - Acc: 0.7365, IoU: 0.4327\n",
      "Class  8 - Acc: 0.9072, IoU: 0.8488\n",
      "Class  9 - Acc: 0.7428, IoU: 0.5167\n",
      "Class 10 - Acc: 0.9357, IoU: 0.8820\n",
      "Class 11 - Acc: 0.7428, IoU: 0.6246\n",
      "Class 12 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 13 - Acc: 0.8743, IoU: 0.8303\n",
      "Class 14 - Acc: 1.0000, IoU: 0.0000\n",
      "Class 15 - Acc: 0.2370, IoU: 0.0000\n",
      "Class 16 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.5945, IoU: 0.4819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:20<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n",
      "Loss: 0.2738\n",
      "Pixel Accuracy: 0.9188\n",
      "Mean Class Accuracy: 0.6915\n",
      "Mean IoU: 0.4599\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9720, IoU: 0.9507\n",
      "Class  1 - Acc: 0.8408, IoU: 0.7243\n",
      "Class  2 - Acc: 0.9157, IoU: 0.8722\n",
      "Class  3 - Acc: 0.6937, IoU: 0.4050\n",
      "Class  4 - Acc: 0.7141, IoU: 0.4955\n",
      "Class  5 - Acc: 0.6254, IoU: 0.2974\n",
      "Class  6 - Acc: 0.7853, IoU: 0.2506\n",
      "Class  7 - Acc: 0.7617, IoU: 0.4780\n",
      "Class  8 - Acc: 0.9142, IoU: 0.8586\n",
      "Class  9 - Acc: 0.7475, IoU: 0.5401\n",
      "Class 10 - Acc: 0.9407, IoU: 0.8910\n",
      "Class 11 - Acc: 0.7539, IoU: 0.6411\n",
      "Class 12 - Acc: 0.7475, IoU: 0.0004\n",
      "Class 13 - Acc: 0.8794, IoU: 0.8383\n",
      "Class 14 - Acc: 0.7057, IoU: 0.0028\n",
      "Class 15 - Acc: 0.5432, IoU: 0.0004\n",
      "Class 16 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 17 - Acc: 0.0000, IoU: 0.0000\n",
      "Class 18 - Acc: 0.5969, IoU: 0.4914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:20<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n",
      "Loss: 0.2501\n",
      "Pixel Accuracy: 0.9231\n",
      "Mean Class Accuracy: 0.7839\n",
      "Mean IoU: 0.4889\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9732, IoU: 0.9529\n",
      "Class  1 - Acc: 0.8523, IoU: 0.7382\n",
      "Class  2 - Acc: 0.9199, IoU: 0.8778\n",
      "Class  3 - Acc: 0.7037, IoU: 0.4368\n",
      "Class  4 - Acc: 0.7293, IoU: 0.5157\n",
      "Class  5 - Acc: 0.6461, IoU: 0.3227\n",
      "Class  6 - Acc: 0.7421, IoU: 0.3254\n",
      "Class  7 - Acc: 0.7762, IoU: 0.5007\n",
      "Class  8 - Acc: 0.9195, IoU: 0.8659\n",
      "Class  9 - Acc: 0.7566, IoU: 0.5598\n",
      "Class 10 - Acc: 0.9416, IoU: 0.8931\n",
      "Class 11 - Acc: 0.7689, IoU: 0.6583\n",
      "Class 12 - Acc: 0.7301, IoU: 0.0495\n",
      "Class 13 - Acc: 0.8960, IoU: 0.8560\n",
      "Class 14 - Acc: 0.5891, IoU: 0.1150\n",
      "Class 15 - Acc: 0.5989, IoU: 0.1243\n",
      "Class 16 - Acc: 0.7992, IoU: 0.0043\n",
      "Class 17 - Acc: 0.9621, IoU: 0.0001\n",
      "Class 18 - Acc: 0.5884, IoU: 0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:20<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n",
      "Loss: 0.2322\n",
      "Pixel Accuracy: 0.9277\n",
      "Mean Class Accuracy: 0.7966\n",
      "Mean IoU: 0.5329\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9750, IoU: 0.9556\n",
      "Class  1 - Acc: 0.8591, IoU: 0.7504\n",
      "Class  2 - Acc: 0.9254, IoU: 0.8848\n",
      "Class  3 - Acc: 0.7436, IoU: 0.4877\n",
      "Class  4 - Acc: 0.7582, IoU: 0.5637\n",
      "Class  5 - Acc: 0.6597, IoU: 0.3410\n",
      "Class  6 - Acc: 0.7186, IoU: 0.3561\n",
      "Class  7 - Acc: 0.7838, IoU: 0.5196\n",
      "Class  8 - Acc: 0.9216, IoU: 0.8691\n",
      "Class  9 - Acc: 0.7768, IoU: 0.5870\n",
      "Class 10 - Acc: 0.9446, IoU: 0.8974\n",
      "Class 11 - Acc: 0.7795, IoU: 0.6685\n",
      "Class 12 - Acc: 0.7390, IoU: 0.1863\n",
      "Class 13 - Acc: 0.9096, IoU: 0.8685\n",
      "Class 14 - Acc: 0.6682, IoU: 0.1868\n",
      "Class 15 - Acc: 0.6368, IoU: 0.2739\n",
      "Class 16 - Acc: 0.8174, IoU: 0.1717\n",
      "Class 17 - Acc: 0.9026, IoU: 0.0455\n",
      "Class 18 - Acc: 0.6159, IoU: 0.5121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:19<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n",
      "Loss: 0.2147\n",
      "Pixel Accuracy: 0.9327\n",
      "Mean Class Accuracy: 0.7997\n",
      "Mean IoU: 0.5857\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9769, IoU: 0.9588\n",
      "Class  1 - Acc: 0.8701, IoU: 0.7666\n",
      "Class  2 - Acc: 0.9299, IoU: 0.8892\n",
      "Class  3 - Acc: 0.7605, IoU: 0.5222\n",
      "Class  4 - Acc: 0.7869, IoU: 0.5979\n",
      "Class  5 - Acc: 0.6726, IoU: 0.3623\n",
      "Class  6 - Acc: 0.7266, IoU: 0.3846\n",
      "Class  7 - Acc: 0.7954, IoU: 0.5436\n",
      "Class  8 - Acc: 0.9254, IoU: 0.8750\n",
      "Class  9 - Acc: 0.8002, IoU: 0.6197\n",
      "Class 10 - Acc: 0.9459, IoU: 0.9000\n",
      "Class 11 - Acc: 0.7977, IoU: 0.6855\n",
      "Class 12 - Acc: 0.7064, IoU: 0.3347\n",
      "Class 13 - Acc: 0.9270, IoU: 0.8860\n",
      "Class 14 - Acc: 0.6848, IoU: 0.3512\n",
      "Class 15 - Acc: 0.5764, IoU: 0.3170\n",
      "Class 16 - Acc: 0.7761, IoU: 0.3217\n",
      "Class 17 - Acc: 0.8636, IoU: 0.2589\n",
      "Class 18 - Acc: 0.6728, IoU: 0.5532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [02:20<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n",
      "Loss: 0.1991\n",
      "Pixel Accuracy: 0.9366\n",
      "Mean Class Accuracy: 0.8118\n",
      "Mean IoU: 0.6237\n",
      "\n",
      "Per-class metrics:\n",
      "Class  0 - Acc: 0.9780, IoU: 0.9601\n",
      "Class  1 - Acc: 0.8750, IoU: 0.7763\n",
      "Class  2 - Acc: 0.9349, IoU: 0.8955\n",
      "Class  3 - Acc: 0.7752, IoU: 0.5582\n",
      "Class  4 - Acc: 0.7906, IoU: 0.6049\n",
      "Class  5 - Acc: 0.6852, IoU: 0.3806\n",
      "Class  6 - Acc: 0.7266, IoU: 0.4059\n",
      "Class  7 - Acc: 0.7930, IoU: 0.5532\n",
      "Class  8 - Acc: 0.9298, IoU: 0.8823\n",
      "Class  9 - Acc: 0.8174, IoU: 0.6443\n",
      "Class 10 - Acc: 0.9486, IoU: 0.9030\n",
      "Class 11 - Acc: 0.8080, IoU: 0.6962\n",
      "Class 12 - Acc: 0.6959, IoU: 0.3880\n",
      "Class 13 - Acc: 0.9354, IoU: 0.8954\n",
      "Class 14 - Acc: 0.7078, IoU: 0.4313\n",
      "Class 15 - Acc: 0.6869, IoU: 0.4285\n",
      "Class 16 - Acc: 0.7975, IoU: 0.4008\n",
      "Class 17 - Acc: 0.8096, IoU: 0.4573\n",
      "Class 18 - Acc: 0.7282, IoU: 0.5883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a simple decoder network\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "class SegmentationDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels=2048, num_classes=19):\n",
    "        super().__init__()\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            # 16x32 -> 32x64\n",
    "            torch.nn.ConvTranspose2d(in_channels, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(1024),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 32x64 -> 64x128\n",
    "            torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 64x128 -> 128x256\n",
    "            torch.nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 128x256 -> 256x512\n",
    "            torch.nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 256x512 -> 512x1024\n",
    "            torch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Final 1x1 conv to get to num_classes\n",
    "            torch.nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        # Ensure exact output size\n",
    "        if x.shape[-2:] != (512, 1024):\n",
    "            x = torch.nn.functional.interpolate(\n",
    "                x, size=(512, 1024), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "        return x\n",
    "\n",
    "# Initialize decoder, optimizer, and loss function\n",
    "decoder = SegmentationDecoder().to(device)\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "def fast_hist(a: np.ndarray, b: np.ndarray, n: int) -> np.ndarray:\n",
    "    k = (b >= 0) & (b < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "def per_class_iou(hist: np.ndarray) -> np.ndarray:\n",
    "    epsilon = 1e-5\n",
    "    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
    "\n",
    "def train_epoch(encoder, decoder, dataloader, optimizer, criterion, device, num_classes=19):\n",
    "    decoder.train()\n",
    "    encoder.train()  # Keep DINO frozen\n",
    "    \n",
    "    total_loss = 0\n",
    "    hist = np.zeros((num_classes, num_classes))  # Single histogram for entire epoch\n",
    "    total_pixels = 0\n",
    "    correct_pixels = 0\n",
    "    \n",
    "    for images, labels in tqdm.tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get DINO features\n",
    "        # with torch.no_grad():\n",
    "        features = encoder(images)[\"res5\"]\n",
    "        \n",
    "        # Forward pass through decoder\n",
    "        outputs = decoder(features)\n",
    "        \n",
    "        # Resize outputs to match label size if needed\n",
    "        if outputs.shape[-2:] != labels.shape[-2:]:\n",
    "            outputs = torch.nn.functional.interpolate(\n",
    "                outputs, size=labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "        \n",
    "        # Pixel Accuracy\n",
    "        valid_mask = labels != 255  # Ignore index\n",
    "        total_pixels += valid_mask.sum().item()\n",
    "        correct_pixels += ((preds == labels) & valid_mask).sum().item()\n",
    "        \n",
    "        # IoU\n",
    "        preds = preds.cpu().numpy()\n",
    "        target = labels.cpu().numpy()\n",
    "        hist += fast_hist(preds.flatten(), target.flatten(), num_classes)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    pixel_acc = correct_pixels / total_pixels\n",
    "    \n",
    "    # Per-class accuracy (mean class accuracy)\n",
    "    class_acc = np.diag(hist) / (hist.sum(1) + np.finfo(np.float32).eps)\n",
    "    mean_class_acc = np.nanmean(class_acc)\n",
    "    \n",
    "    # IoU metrics\n",
    "    iou = per_class_iou(hist)\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'pixel_acc': pixel_acc,\n",
    "        'mean_class_acc': mean_class_acc,\n",
    "        'mean_iou': mean_iou,\n",
    "        'class_iou': iou,\n",
    "        'class_acc': class_acc\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    CITYSCAPES_dataset, \n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    metrics = train_epoch(encoder, decoder, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Loss: {metrics['loss']:.4f}\")\n",
    "    print(f\"Pixel Accuracy: {metrics['pixel_acc']:.4f}\")\n",
    "    print(f\"Mean Class Accuracy: {metrics['mean_class_acc']:.4f}\")\n",
    "    print(f\"Mean IoU: {metrics['mean_iou']:.4f}\")\n",
    "    \n",
    "    # Optionally print per-class metrics\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for i in range(19):  # Assuming 19 classes\n",
    "        print(f\"Class {i:2d} - Acc: {metrics['class_acc'][i]:.4f}, IoU: {metrics['class_iou'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1867324368.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Epoch 10/10\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Epoch 10/10\n",
    "Loss: 0.1003\n",
    "Pixel Accuracy: 0.9679\n",
    "Mean Class Accuracy: 0.8727\n",
    "Mean IoU: 0.7644\n",
    "\n",
    "Per-class metrics:\n",
    "Class  0 - Acc: 0.9918, IoU: 0.9840\n",
    "Class  1 - Acc: 0.9472, IoU: 0.9003\n",
    "Class  2 - Acc: 0.9699, IoU: 0.9463\n",
    "Class  3 - Acc: 0.9133, IoU: 0.8134\n",
    "Class  4 - Acc: 0.9030, IoU: 0.8098\n",
    "Class  5 - Acc: 0.8038, IoU: 0.5960\n",
    "Class  6 - Acc: 0.7877, IoU: 0.5778\n",
    "Class  7 - Acc: 0.8654, IoU: 0.7161\n",
    "Class  8 - Acc: 0.9620, IoU: 0.9328\n",
    "Class  9 - Acc: 0.9049, IoU: 0.8156\n",
    "Class 10 - Acc: 0.9672, IoU: 0.9387\n",
    "Class 11 - Acc: 0.8874, IoU: 0.8046\n",
    "Class 12 - Acc: 0.7999, IoU: 0.6324\n",
    "Class 13 - Acc: 0.9691, IoU: 0.9456\n",
    "Class 14 - Acc: 0.9386, IoU: 0.8745\n",
    "Class 15 - Acc: 0.6084, IoU: 0.2200\n",
    "Class 16 - Acc: 0.6444, IoU: 0.5633\n",
    "Class 17 - Acc: 0.8798, IoU: 0.7277\n",
    "Class 18 - Acc: 0.8375, IoU: 0.7250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated_student_map shape: torch.Size([2, 256, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arda/tmp/ipykernel_2938576/3080879353.py:73: UserWarning: Using a target size (torch.Size([2, 768, 16, 16])) that is different to the input size (torch.Size([2, 256, 16, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(F.normalize(updated_student_map, dim=1),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (768) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 73\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# => [2, 256, 16, 16]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Distillation loss (example)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# You'd typically need to align teacher_map shape or do some pooling, \u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# but let's just do a simple MSE with naive upsampling of teacher_map:\u001b[39;00m\n\u001b[1;32m     72\u001b[0m teacher_map_upsampled \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(teacher_map, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_student_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_map_upsampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistillation loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (768) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttentionDistiller(nn.Module):\n",
    "    def __init__(self, \n",
    "                 student_dim,   # e.g., 256 or 512\n",
    "                 teacher_dim,   # e.g., 768\n",
    "                 hidden_dim,    # dimension for cross-attention\n",
    "                 num_heads=8):\n",
    "        super().__init__()\n",
    "        # Linear projections to match dimension\n",
    "        self.student_proj = nn.Linear(student_dim, hidden_dim, bias=False)\n",
    "        self.teacher_proj = nn.Linear(teacher_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # Using PyTorch's multi-head attention\n",
    "        # batch_first=True means input shape is [B, seq_len, dim]\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden_dim, \n",
    "                                                num_heads=num_heads, \n",
    "                                                batch_first=True)\n",
    "        \n",
    "        # Optional final projection back to student_dim\n",
    "        self.proj_back = nn.Linear(hidden_dim, student_dim, bias=False)\n",
    "\n",
    "    def forward(self, student_map, teacher_map):\n",
    "        \"\"\"\n",
    "        student_map: [B, C_s, H_s, W_s]\n",
    "        teacher_map: [B, C_t, H_t, W_t]\n",
    "        returns cross_attended_map: [B, C_s, H_s, W_s] (updated student features)\n",
    "        \"\"\"\n",
    "        B, C_s, H_s, W_s = student_map.shape\n",
    "        _, C_t, H_t, W_t = teacher_map.shape\n",
    "        \n",
    "        # Flatten [B, C_s, H_s, W_s] -> [B, H_s*W_s, C_s], then project\n",
    "        student_tokens = student_map.permute(0,2,3,1).reshape(B, H_s*W_s, C_s)\n",
    "        student_tokens = self.student_proj(student_tokens)  # => [B, H_s*W_s, hidden_dim]\n",
    "        \n",
    "        # Flatten teacher -> [B, H_t*W_t, C_t], then project\n",
    "        teacher_tokens = teacher_map.permute(0,2,3,1).reshape(B, H_t*W_t, C_t)\n",
    "        teacher_tokens = self.teacher_proj(teacher_tokens)  # => [B, H_t*W_t, hidden_dim]\n",
    "        \n",
    "        # Cross-Attention: Q=student, K=teacher, V=teacher\n",
    "        cross_attended, _ = self.cross_attn(query=student_tokens,\n",
    "                                            key=teacher_tokens,\n",
    "                                            value=teacher_tokens)\n",
    "        \n",
    "        # Project back to student dimension if needed\n",
    "        cross_attended = self.proj_back(cross_attended)  # [B, H_s*W_s, C_s]\n",
    "        \n",
    "        # Reshape back to [B, C_s, H_s, W_s]\n",
    "        cross_attended_map = cross_attended.view(B, H_s, W_s, C_s).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return cross_attended_map\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    B = 2\n",
    "    student_map = torch.randn(B, 256, 16, 16)  # e.g., [B, C_s, H_s, W_s]\n",
    "    teacher_map = torch.randn(B, 768, 8, 8)    # [B, C_t, H_t, W_t]\n",
    "    \n",
    "    # Suppose we choose hidden_dim=384\n",
    "    distiller = CrossAttentionDistiller(student_dim=256, teacher_dim=768, hidden_dim=384, num_heads=6)\n",
    "    updated_student_map = distiller(student_map, teacher_map)\n",
    "    \n",
    "    print(\"updated_student_map shape:\", updated_student_map.shape)\n",
    "    # => [2, 256, 16, 16]\n",
    "    \n",
    "    # Distillation loss (example)\n",
    "    # You'd typically need to align teacher_map shape or do some pooling, \n",
    "    # but let's just do a simple MSE with naive upsampling of teacher_map:\n",
    "    teacher_map_upsampled = F.interpolate(teacher_map, size=(16,16), mode='bilinear')\n",
    "    loss = F.mse_loss(F.normalize(updated_student_map, dim=1),\n",
    "                      F.normalize(teacher_map_upsampled, dim=1))\n",
    "    print(\"distillation loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
