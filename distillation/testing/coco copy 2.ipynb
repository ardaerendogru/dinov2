{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "class CocoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for COCO segmentation dataset.\n",
    "    \n",
    "    This class handles loading and preprocessing of COCO images and their corresponding\n",
    "    segmentation masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir: str, annotation_file: str, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the COCO dataset.\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the images\n",
    "            annotation_file (str): Path to COCO annotation JSON file\n",
    "            transform: Optional transform to be applied to images and masks\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Get an image and its corresponding segmentation mask.\n",
    "        \n",
    "        Args:\n",
    "            index (int): Index of the data point\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, mask) where image is a normalized tensor and \n",
    "                  mask is a tensor containing segmentation labels\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        img_id = self.ids[index]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load mask\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.int32)\n",
    "        for ann in anns:\n",
    "            mask = np.maximum(mask, self.coco.annToMask(ann) * ann['category_id'])\n",
    "            \n",
    "        # Convert to tensors\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=np.array(img), mask=mask)\n",
    "            img, mask = transformed['image'], transformed['mask']\n",
    "            \n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        # Map background (0) to 255 and decrease other class IDs by 1\n",
    "        # mask[mask == 0] = 255\n",
    "        # mask[mask < 255] -= 1\n",
    "        \n",
    "        return img, mask\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of images in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: Length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.64s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# For training data\n",
    "import albumentations as A\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(504, 504),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = CocoDataset(\n",
    "    root_dir='/home/arda/anyma/datasets/coco/train2017',\n",
    "    annotation_file='/home/arda/anyma/datasets/coco/annotations/instances_train2017.json',\n",
    "    transform=transform  # Add your transforms here if needed\n",
    ")\n",
    "\n",
    "# For validation data\n",
    "val_dataset = CocoDataset(\n",
    "    root_dir='/home/arda/anyma/datasets/coco/val2017',\n",
    "    annotation_file='/home/arda/anyma/datasets/coco/annotations/instances_val2017.json',\n",
    "    transform=transform  # Add your transforms here if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/arda/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/arda/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/arda/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/arda/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/arda/dinov2/distillation')\n",
    "from models.dinov2 import DINOv2ViT\n",
    "device = 'cuda' \n",
    "encoder = DINOv2ViT().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 1957/3697 [1:36:47<1:25:21,  2.94s/it]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=8)\n",
    "\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Starting from 36x36 (after encoder)\n",
    "            nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=2, padding=1),  # -> 72x72\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # -> 144x144\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # -> 288x288\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # -> 576x576\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=(504, 504), mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(32, num_classes, kernel_size=1)  # 1x1 conv for final class predictions\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)  # Remove the double application of decoder\n",
    "\n",
    "# Initialize models and training components\n",
    "num_classes = 91  # COCO has 90 classes + background\n",
    "decoder = SegmentationHead(in_channels=1536, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = optim.Adam(list(decoder.parameters())+list(encoder.parameters()), lr=1e-4)\n",
    "scaler = GradScaler(device=device)\n",
    "\n",
    "def fast_hist(a: np.ndarray, b: np.ndarray, n: int) -> np.ndarray:\n",
    "    k = (b >= 0) & (b < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "def per_class_iou(hist: np.ndarray) -> np.ndarray:\n",
    "    epsilon = 1e-5\n",
    "    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
    "\n",
    "# /home/arda/resnet_logs/v2/coco.ipynb\n",
    "def train_one_epoch(encoder, decoder, dataloader, criterion, optimizer, scaler):\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    hist = np.zeros((num_classes, num_classes))\n",
    "    total_pixels = 0\n",
    "    correct_pixels = 0\n",
    "    \n",
    "    for images, masks in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "            # Ensure encoder runs in float32 without autocast\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            features = encoder(images)['feature_map']\n",
    "\n",
    "            outputs = decoder(features)\n",
    "            loss = criterion(outputs, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Pixel Accuracy\n",
    "        valid_mask = masks != 255\n",
    "        total_pixels += valid_mask.sum().item()\n",
    "        correct_pixels += ((preds == masks) & valid_mask).sum().item()\n",
    "        \n",
    "        # IoU\n",
    "        preds = preds.cpu().numpy()\n",
    "        target = masks.cpu().numpy()\n",
    "        hist += fast_hist(preds.flatten(), target.flatten(), num_classes)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    pixel_acc = correct_pixels / total_pixels\n",
    "    class_acc = np.diag(hist) / (hist.sum(1) + np.finfo(np.float32).eps)\n",
    "    mean_class_acc = np.nanmean(class_acc)\n",
    "    iou = per_class_iou(hist)\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'pixel_acc': pixel_acc,\n",
    "        'mean_class_acc': mean_class_acc,\n",
    "        'mean_iou': mean_iou,\n",
    "        'class_iou': iou,\n",
    "        'class_acc': class_acc\n",
    "    }\n",
    "\n",
    "# /home/arda/resnet_logs/v2/coco.ipynb\n",
    "# /home/arda/resnet_logs/v2/coco.ipynb\n",
    "def validate(encoder, decoder, dataloader, criterion):\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    hist = np.zeros((num_classes, num_classes))\n",
    "    total_pixels = 0\n",
    "    correct_pixels = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Ensure encoder runs in float32 without autocast\n",
    "            \n",
    "            with autocast(device_type='cuda'):\n",
    "                features = encoder(images)['feature_map']\n",
    "                outputs = decoder(features)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            valid_mask = masks != 255\n",
    "            total_pixels += valid_mask.sum().item()\n",
    "            correct_pixels += ((preds == masks) & valid_mask).sum().item()\n",
    "            \n",
    "            preds = preds.cpu().numpy()\n",
    "            target = masks.cpu().numpy()\n",
    "            hist += fast_hist(preds.flatten(), target.flatten(), num_classes)\n",
    "    \n",
    "    pixel_acc = correct_pixels / total_pixels\n",
    "    class_acc = np.diag(hist) / (hist.sum(1) + np.finfo(np.float32).eps)\n",
    "    mean_class_acc = np.nanmean(class_acc)\n",
    "    iou = per_class_iou(hist)\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'pixel_acc': pixel_acc,\n",
    "        'mean_class_acc': mean_class_acc,\n",
    "        'mean_iou': mean_iou,\n",
    "        'class_iou': iou,\n",
    "        'class_acc': class_acc\n",
    "    }\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_miou = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_metrics = train_one_epoch(encoder, decoder, train_loader, criterion, optimizer, scaler)\n",
    "    val_metrics = validate(encoder, decoder, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Train - Loss: {train_metrics['loss']:.4f}, Pixel Acc: {train_metrics['pixel_acc']:.4f}, \"\n",
    "          f\"Mean IoU: {train_metrics['mean_iou']:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Pixel Acc: {val_metrics['pixel_acc']:.4f}, \"\n",
    "          f\"Mean IoU: {val_metrics['mean_iou']:.4f}\")\n",
    "    \n",
    "    # Save best model based on validation mIoU\n",
    "    if val_metrics['mean_iou'] > best_val_miou:\n",
    "        best_val_miou = val_metrics['mean_iou']\n",
    "        torch.save({\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'metrics': val_metrics\n",
    "        }, 'best_segmentation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
